{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import Tensor\n",
    "import torch.optim as optim\n",
    "import torch.utils.data\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.utils as vutils\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "from utils import set_random_seed, load_data, CreateDataLoader\n",
    "\n",
    "from Generator import Generator\n",
    "\n",
    "import config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting seeds ...... \n",
      "\n"
     ]
    }
   ],
   "source": [
    "set_random_seed(config.RANDOM_SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'DCGAN/CIFAR10'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PATH = f'DCGAN/{config.USED_DATA}'\n",
    "PATH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "if config.USED_DATA == \"CIFAR10\":\n",
    "\ttransform = transforms.Compose([\n",
    "\t\t\t\t\t\t\t   transforms.Resize(64),\n",
    "\t\t\t\t\t\t\t   transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
    "\t\t\t\t\t\t   ])\n",
    "\tn_channels = 3\n",
    "elif config.USED_DATA == \"MNIST\" or config.USED_DATA == \"DOODLE\":\n",
    "\ttransform = transforms.Compose([\n",
    "\t\t\t\t\t\t\t   transforms.Resize(64),\n",
    "\t\t\t\t\t\t\t   transforms.Normalize([0.5], [0.5]),\n",
    "\t\t\t\t\t\t   ])\n",
    "\tn_channels = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<utils.CustomDataSet at 0x7f4e5c03db80>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset, *_ = load_data(transform)\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "latent_size = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader = CreateDataLoader(dataset, batch_size = config.GAN_BATCH_SIZE, device = config.DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weights_init(m):\n",
    "\tclassname = m.__class__.__name__\n",
    "\tif classname.find('Conv') != -1:\n",
    "\t\tm.weight.data.normal_(0.0, 0.02)\n",
    "\telif classname.find('BatchNorm') != -1:\n",
    "\t\tm.weight.data.normal_(1.0, 0.02)\n",
    "\t\tm.bias.data.fill_(0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Generator(\n",
       "  (initial): TransposeBN(\n",
       "    (deConv): ConvTranspose2d(100, 512, kernel_size=(4, 4), stride=(1, 1), bias=False)\n",
       "    (Bn): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (act): ReLU(inplace=True)\n",
       "  )\n",
       "  (Transposed): Sequential(\n",
       "    (0): TransposeBN(\n",
       "      (deConv): ConvTranspose2d(512, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (Bn): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (act): ReLU(inplace=True)\n",
       "    )\n",
       "    (1): TransposeBN(\n",
       "      (deConv): ConvTranspose2d(256, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (Bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (act): ReLU(inplace=True)\n",
       "    )\n",
       "    (2): TransposeBN(\n",
       "      (deConv): ConvTranspose2d(128, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (Bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (act): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (out): ConvTranspose2d(64, 3, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "  (tanh): Tanh()\n",
       ")"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "netG = Generator(latent_size, n_channels).to(config.DEVICE)\n",
    "netG.apply(weights_init)\n",
    "netG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discriminator(nn.Module):\n",
    "\tdef __init__(self, n_channels, filters = [64, 128, 256, 512]):\n",
    "\t\tsuper(Discriminator, self).__init__()\n",
    "\t\tself.main = nn.Sequential(\n",
    "\t\t\t# input is (nc) x 64 x 64\n",
    "\t\t\tnn.Conv2d(n_channels, filters[0], 4, 2, 1, bias=False),\n",
    "\t\t\tnn.LeakyReLU(0.2, inplace=True),\n",
    "\t\t\t# state size. (ndf) x 32 x 32\n",
    "\t\t\tnn.Conv2d(filters[0], filters[1], 4, 2, 1, bias=False),\n",
    "\t\t\tnn.BatchNorm2d(filters[1]),\n",
    "\t\t\tnn.LeakyReLU(0.2, inplace=True),\n",
    "\t\t\t# state size. (ndf*2) x 16 x 16\n",
    "\t\t\tnn.Conv2d(filters[1], filters[2], 4, 2, 1, bias=False),\n",
    "\t\t\tnn.BatchNorm2d(filters[2]),\n",
    "\t\t\tnn.LeakyReLU(0.2, inplace=True),\n",
    "\t\t\t# state size. (ndf*4) x 8 x 8\n",
    "\t\t\tnn.Conv2d(filters[2], filters[3], 4, 2, 1, bias=False),\n",
    "\t\t\tnn.BatchNorm2d(filters[3]),\n",
    "\t\t\tnn.LeakyReLU(0.2, inplace=True),\n",
    "\t\t\t# state size. (ndf*8) x 4 x 4\n",
    "\t\t\tnn.Conv2d(filters[3], 1, 4, 1, 0, bias=False)\n",
    "\t\t)\n",
    "\n",
    "\tdef forward(self, X: Tensor):\n",
    "\t\tout = self.main(X)\n",
    "\n",
    "\t\treturn out.view(-1, 1).squeeze(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Discriminator(\n",
       "  (main): Sequential(\n",
       "    (0): Conv2d(3, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "    (1): LeakyReLU(negative_slope=0.2, inplace=True)\n",
       "    (2): Conv2d(64, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "    (3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (4): LeakyReLU(negative_slope=0.2, inplace=True)\n",
       "    (5): Conv2d(128, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "    (6): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (7): LeakyReLU(negative_slope=0.2, inplace=True)\n",
       "    (8): Conv2d(256, 512, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "    (9): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (10): LeakyReLU(negative_slope=0.2, inplace=True)\n",
       "    (11): Conv2d(512, 1, kernel_size=(4, 4), stride=(1, 1), bias=False)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "netD = Discriminator(n_channels).to(config.DEVICE)\n",
    "netD.apply(weights_init)\n",
    "netD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.BCEWithLogitsLoss()\n",
    "# optimizerD = optim.Adam(netD.parameters(), lr = 0.0002, betas = [0.5, 0.999])\n",
    "# optimizerG = optim.Adam(netG.parameters(), lr = 0.0002, betas = [0.5, 0.999])\n",
    "\n",
    "optimizerD = optim.RMSprop(netD.parameters(), lr = 0.0001)\n",
    "optimizerG = optim.RMSprop(netG.parameters(), lr = 0.0002)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "fixed_noise = torch.randn(64, latent_size, 1, 1, device=config.DEVICE)\n",
    "real_label = 1\n",
    "fake_label = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4442009a46c149a8b8e553f1e1db22bb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/391 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10/9] Loss_D: 1.2477 Loss_G: 0.7284 D(x): -0.3896 D(G(z)): -1.1933 / -0.015266\r"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "76ce2a7679a14ca0878dad92d037e100",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/391 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[11/9] Loss_D: 1.3384 Loss_G: 1.3587 D(x): 0.5868 D(G(z)): 0.2050 / -0.967676\r"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9f8612672a8c46a1b369e5542c8ccf53",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/391 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12/9] Loss_D: 1.4899 Loss_G: 2.0324 D(x): 1.1621 D(G(z)): 0.7984 / -1.856453\r"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "143c7bf994ba4a83981021756130ea61",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/391 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13/9] Loss_D: 1.3859 Loss_G: 1.7408 D(x): 0.9505 D(G(z)): 0.5503 / -1.518559\r"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f765f1461b2144ecbf6d74ea5fdedc24",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/391 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[14/9] Loss_D: 1.1812 Loss_G: 0.8847 D(x): -0.1502 D(G(z)): -0.8481 / -0.3072\r"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "18f10456c1214c55b8703830755c034f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/391 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[15/9] Loss_D: 1.0488 Loss_G: 0.8106 D(x): -0.0452 D(G(z)): -1.0969 / -0.1733\r"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 28\u001b[0m\n\u001b[1;32m     26\u001b[0m errD_fake \u001b[38;5;241m=\u001b[39m criterion(output, label)\n\u001b[1;32m     27\u001b[0m errD_fake\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[0;32m---> 28\u001b[0m D_G_z1 \u001b[38;5;241m=\u001b[39m \u001b[43moutput\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmean\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     29\u001b[0m errD \u001b[38;5;241m=\u001b[39m errD_real \u001b[38;5;241m+\u001b[39m errD_fake\n\u001b[1;32m     30\u001b[0m optimizerD\u001b[38;5;241m.\u001b[39mstep()\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "niter = 10\n",
    "g_loss = []\n",
    "d_loss = []\n",
    "\n",
    "for epoch in range(10, 10 + niter):\n",
    "\tfor batch in tqdm(dataloader, leave = True, position=0):\n",
    "\t\timages, _ = batch\n",
    "\t\tbatch_size = images.size(0)\n",
    "\t\t############################\n",
    "\t\t# (1) Update D network: maximize log(D(x)) + log(1 - D(G(z)))\n",
    "\t\t###########################\n",
    "\t\t# train with real\n",
    "\t\tnetD.zero_grad()\n",
    "\t\tlabel = torch.full((batch_size,), real_label, device=config.DEVICE, dtype=torch.float)\n",
    "\n",
    "\t\toutput = netD(images)\n",
    "\t\terrD_real = criterion(output, label)\n",
    "\t\terrD_real.backward()\n",
    "\t\tD_x = output.mean().item()\n",
    "\n",
    "\t\t# train with fake\n",
    "\t\tnoise = torch.randn(batch_size, latent_size, 1, 1, device=config.DEVICE)\n",
    "\t\tfake = netG(noise)\n",
    "\t\tlabel.fill_(fake_label)\n",
    "\t\toutput = netD(fake.detach())\n",
    "\t\terrD_fake = criterion(output, label)\n",
    "\t\terrD_fake.backward()\n",
    "\t\tD_G_z1 = output.mean().item()\n",
    "\t\terrD = errD_real + errD_fake\n",
    "\t\toptimizerD.step()\n",
    "\n",
    "\t\t############################\n",
    "\t\t# (2) Update G network: maximize log(D(G(z)))\n",
    "\t\t###########################\n",
    "\t\tnetG.zero_grad()\n",
    "\t\tlabel.fill_(real_label)  # fake labels are real for generator cost\n",
    "\t\toutput = netD(fake)\n",
    "\t\terrG = criterion(output, label)\n",
    "\t\terrG.backward()\n",
    "\t\tD_G_z2 = output.mean().item()\n",
    "\t\toptimizerG.step()\n",
    "\n",
    "\t\ts = '[%d/%d] Loss_D: %.4f Loss_G: %.4f D(x): %.4f D(G(z)): %.4f / %.4f' % (epoch, niter-1, errD.item(), errG.item(), D_x, D_G_z1, D_G_z2)\n",
    "\t\t\n",
    "\t\ttqdm.write(s, end = \"\\r\")\n",
    "\t\t\n",
    "\tfake = netG(fixed_noise)\n",
    "\tvutils.save_image(fake.detach(),f'{PATH}/fake_samples_epoch_{epoch:03d}.png', normalize=True)\n",
    "\t\t\n",
    "\t\n",
    "\t\n",
    "\t# Check pointing for every epoch\n",
    "\ttorch.save(netG.state_dict(), f'{PATH}/netG_epoch_{epoch:03d}.pt')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "GAN",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
