{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch import Tensor\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision.datasets import MNIST\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from CNNmodel import ConvModel\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_seed = 0\n",
    "torch.manual_seed(random_seed)\n",
    "np.random.seed(random_seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = \"./data\"\n",
    "n_classes = 10\n",
    "num_labelled = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot(y):\n",
    "\tone_hot_y = torch.zeros((len(y), 10))\n",
    "\tone_hot_y[np.arange(len(y)), y] = 1\n",
    "\treturn one_hot_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = MNIST(DATA_DIR, train = True, download=True)\n",
    "data_X = data.data.unsqueeze(1).float()\n",
    "data_X /= 255\n",
    "data_Y = one_hot(data.targets)\n",
    "\n",
    "num_data = data_Y.numel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def supervised_samples(X: Tensor, y: Tensor, n_samples, n_classes=10):\n",
    "\tnumber_per_class = n_samples//n_classes\n",
    "\n",
    "\n",
    "\tX_sup, y_sup = Tensor(), Tensor()\n",
    "\n",
    "\n",
    "\tfor i in range(n_classes):\n",
    "\t\tX_with_class = X[torch.argmax(y, 1) == i]\n",
    "\n",
    "\t\tix = torch.randint(0, len(X_with_class), [number_per_class])\n",
    "\n",
    "\t\tX_sup = torch.cat((X_sup, X_with_class[ix]))\n",
    "\n",
    "\t\ty_sup = torch.cat((y_sup, one_hot([i]*number_per_class)))\n",
    "\n",
    "\treturn X_sup, y_sup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataSet(Dataset):\n",
    "\tdef __init__(self, x, y) -> None:\n",
    "\t\tself.x: torch.Tensor = x\n",
    "\t\tself.y: torch.Tensor = y\n",
    "\t\tself.n_samples = len(y)\n",
    "\t\n",
    "\tdef __getitem__(self, index):\n",
    "\t\treturn self.x[index], self.y[index]\n",
    "\t\n",
    "\tdef __len__(self):\n",
    "\t\treturn self.n_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "\tdef __init__(self, inp_size, out_size) -> None:\n",
    "\t\tsuper().__init__()\n",
    "\n",
    "\n",
    "\t\tself.NN = nn.Sequential(\n",
    "\t\t\tnn.Linear(inp_size, 256*7*7),\n",
    "\t\t\tnn.LeakyReLU(negative_slope=0.2),\n",
    "\t\t)\n",
    "\n",
    "\t\tself.CONV = nn.Sequential(\n",
    "\t\t\tnn.ConvTranspose2d(256, 128, (3, 3), (2, 2)),\n",
    "\t\t\tnn.BatchNorm2d(128),\n",
    "\t\t\tnn.LeakyReLU(0.2),\n",
    "\t\t\tnn.ConvTranspose2d(128, 64, (3, 3), (1, 1)),\n",
    "\t\t\tnn.BatchNorm2d(64),\n",
    "\t\t\tnn.LeakyReLU(0.2),\n",
    "\t\t\t\n",
    "\t\t)\n",
    "\n",
    "\t\tself.out = nn.Sequential(\n",
    "\t\t\tnn.ConvTranspose2d(64, out_size[0], (3, 3), (2, 2)),\n",
    "\t\t\tnn.AdaptiveAvgPool2d((out_size[1], out_size[2])),\n",
    "\t\t\tnn.Sigmoid()\n",
    "\t\t)\n",
    "\n",
    "\t\tself.optimizer = optim.Adam(self.parameters(), lr = 0.0002, betas=[0.5, 0.999])\n",
    "\t\tself.criterion = nn.BCELoss()\n",
    "\t\n",
    "\tdef forward(self, X: Tensor):\n",
    "\t\tX = self.NN(X)\n",
    "\t\tX = X.view(-1, 256, 7, 7)\n",
    "\t\tX = self.CONV(X)\n",
    "\n",
    "\t\tX = self.out(X)\n",
    "\n",
    "\t\treturn X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Feature_Extractor(nn.Module):\n",
    "\tdef __init__(self, inp_channel) -> None:\n",
    "\t\tsuper().__init__()\n",
    "\n",
    "\n",
    "\t\tself.CNN = ConvModel(inp_channel)\n",
    "\n",
    "\t\tself.dropout = nn.Sequential(\n",
    "\t\t\tnn.Dropout(0.4)\n",
    "\t\t)\n",
    "\n",
    "\t\n",
    "\tdef forward(self, X: Tensor):\n",
    "\t\tX = self.CNN(X)\n",
    "\t\tX = self.dropout(X)\n",
    "\t\treturn X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Classify(nn.Module):\n",
    "\tdef __init__(self, feature_extractor: nn.Module, num_classes) -> None:\n",
    "\t\tsuper().__init__()\n",
    "\n",
    "\t\tself.CNN = feature_extractor\n",
    "\n",
    "\t\tself.out = nn.Sequential(\n",
    "\t\t\tnn.Linear(512, num_classes),\n",
    "\t\t\tnn.Softmax(1)\n",
    "\t\t)\n",
    "\n",
    "\t\tself.optimizer = optim.Adam(self.parameters(), lr = 0.0002, betas= [0.5, 0.999])\n",
    "\n",
    "\t\tself.criterion = nn.BCELoss()\n",
    "\t\n",
    "\t\treturn torch.argmax(self.forward(x)).item()\n",
    "\t\n",
    "\tdef forward(self, X: Tensor):\n",
    "\t\tX = self.CNN(X)\n",
    "\t\tX = self.out(X)\n",
    "\n",
    "\t\treturn X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 679,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discriminator(nn.Module):\n",
    "\tdef __init__(self, feature_extractor) -> None:\n",
    "\t\tsuper().__init__()\n",
    "\t\tself.CNN = feature_extractor\n",
    "\n",
    "\t\tself.out = nn.Sequential(\n",
    "\t\t\tnn.Linear(512, 1),\n",
    "\t\t\tnn.Sigmoid()\n",
    "\t\t)\n",
    "\t\n",
    "\t\tself.optimizer = optim.Adam(self.parameters(), lr=0.0002, betas=[0.5, 0.999])\n",
    "\t\tself.criterion = nn.BCELoss()\n",
    "\n",
    "\tdef forward(self, X: Tensor):\n",
    "\t\tX = self.CNN(X)\n",
    "\n",
    "\t\tX = self.out(X)\n",
    "\n",
    "\t\treturn X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 680,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SGAN:\n",
    "\tdef __init__(self, image_size, num_classes, feature_extractor: nn.Module, latent_size = 100, lr=0.0002):\n",
    "\n",
    "\t\tself.latent_size = latent_size\n",
    "\n",
    "\t\tCNN = feature_extractor\n",
    "\n",
    "\t\tself.generator = Generator(latent_size, image_size)\n",
    "\n",
    "\t\tself.classify = Classify(CNN, num_classes)\n",
    "\t\tself.discriminator = Discriminator(CNN)\n",
    "\n",
    "\t\tself.history = {}\n",
    "\t\n",
    "\tdef __call__(self, X: torch.Tensor):\n",
    "\t\treturn torch.argmax(self.classify(X))\n",
    "\t\n",
    "\tdef validation(self, X: Tensor, y: Tensor):\n",
    "\t\tself.classify.eval()\n",
    "\n",
    "\t\tnum_data = y.shape[0]\n",
    "\n",
    "\t\trun_size = 10000\n",
    "\n",
    "\t\tcurrent = 0\n",
    "\t\tcorrect = 0\n",
    "\t\t\n",
    "\t\twhile current < num_data:\n",
    "\t\t\tcorrect += torch.count_nonzero(torch.argmax(self.classify(X[current: current + run_size]), 1) == torch.argmax(y[current: current + run_size], 1))\n",
    "\t\t\tcurrent += run_size\n",
    "\n",
    "\t\treturn (correct.float() / num_data).item()\n",
    "\n",
    "\n",
    "\tdef fit(self, X: Tensor, y: Tensor, epochs = 100, batch_size = 64):\n",
    "\n",
    "\t\tX_sup, y_sup = supervised_samples(X, y, num_labelled)\n",
    "\n",
    "\t\tbat_per_epo = int(X.shape[0]/batch_size)\n",
    "\n",
    "\t\tn_step = bat_per_epo * epochs\n",
    "\n",
    "\t\thalf_batch = batch_size//2\n",
    "\n",
    "\t\ttrain_acc = 0\n",
    "\n",
    "\t\tfor epoch in range(epochs):\n",
    "\t\t\tself.classify.train()\n",
    "\t\t\tself.discriminator.train()\n",
    "\t\t\tself.generator.train()\n",
    "\n",
    "\t\t\t# classify\n",
    "\n",
    "\t\t\tif train_acc <= 0.90:\n",
    "\n",
    "\t\t\t\tX_sup_real, y_sup_real = real_samples(X_sup, y_sup, half_batch)\n",
    "\t\t\t\tout_sup = self.classify(X_sup_real)\n",
    "\t\t\t\tsup_loss: Tensor = self.classify.criterion(out_sup, y_sup_real)\n",
    "\n",
    "\t\t\t\tself.classify.optimizer.zero_grad()\n",
    "\t\t\t\tsup_loss.backward()\n",
    "\t\t\t\tself.classify.optimizer.step()\n",
    "\n",
    "\t\t\t# discriminator real\n",
    "\t\t\tX_real, _ = real_samples(X, y, half_batch)\n",
    "\n",
    "\t\t\tout_real = self.discriminator(X_real)\n",
    "\t\t\t\n",
    "\n",
    "\t\t\treal_loss: Tensor = self.discriminator.criterion(out_real, torch.ones(half_batch, 1))\n",
    "\n",
    "\t\t\tself.discriminator.optimizer.zero_grad()\n",
    "\t\t\treal_loss.backward()\n",
    "\t\t\tself.classify.optimizer.step()\n",
    "\n",
    "\t\t\tz = torch.randn(half_batch, self.latent_size)\n",
    "\n",
    "\t\t\t# discriminator fake\n",
    "\t\t\tX_fake = self.generator(z)\n",
    "\t\t\tout_fake = self.discriminator(X_fake)\n",
    "\t\t\ty_fake = torch.zeros((half_batch, 1))\n",
    "\t\t\tfake_loss: Tensor = self.discriminator.criterion(out_fake, y_fake)\n",
    "\t\t\tself.discriminator.optimizer.zero_grad()\n",
    "\t\t\tfake_loss.backward()\n",
    "\t\t\tself.discriminator.optimizer.step()\n",
    "\n",
    "\t\t\t# generator\n",
    "\t\t\tz = torch.randn(batch_size, self.latent_size)\n",
    "\t\t\tX_gen = self.generator(z)\n",
    "\t\t\tout_gen = self.discriminator(X_gen)\n",
    "\t\t\ty_gen = torch.ones((batch_size, 1))\n",
    "\t\t\tgen_loss: Tensor = self.generator.criterion(out_gen, y_gen)\n",
    "\n",
    "\t\t\tself.generator.optimizer.zero_grad()\n",
    "\t\t\tgen_loss.backward()\n",
    "\t\t\tself.generator.optimizer.step()\n",
    "\n",
    "\t\t\ttrain_acc = self.validation(X_sup, y_sup)\n",
    "\n",
    "\t\t\tprint(f\"epoch: {epoch}, train acc: {train_acc*100:.2f}%, classification_loss: {sup_loss:.2f}, discrimination_loss: {(real_loss+fake_loss)/2:.2f}, generation_loss: {gen_loss:.2f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 681,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SGAN([1, 28, 28], n_classes, ConvModel(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 682,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, train acc: 10.00%, classification_loss: 0.34, discrimination_loss: 0.75, generation_loss: 0.67\n",
      "epoch: 1, train acc: 10.00%, classification_loss: 0.34, discrimination_loss: 0.73, generation_loss: 0.59\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 2, train acc: 11.00%, classification_loss: 0.30, discrimination_loss: 0.73, generation_loss: 0.68\n",
      "epoch: 3, train acc: 14.00%, classification_loss: 0.30, discrimination_loss: 0.66, generation_loss: 0.68\n",
      "epoch: 4, train acc: 18.00%, classification_loss: 0.31, discrimination_loss: 0.60, generation_loss: 0.71\n",
      "epoch: 5, train acc: 21.00%, classification_loss: 0.28, discrimination_loss: 0.61, generation_loss: 0.73\n",
      "epoch: 6, train acc: 31.00%, classification_loss: 0.30, discrimination_loss: 0.65, generation_loss: 0.63\n",
      "epoch: 7, train acc: 30.00%, classification_loss: 0.29, discrimination_loss: 0.66, generation_loss: 0.56\n",
      "epoch: 8, train acc: 34.00%, classification_loss: 0.28, discrimination_loss: 0.65, generation_loss: 0.65\n",
      "epoch: 9, train acc: 41.00%, classification_loss: 0.28, discrimination_loss: 0.64, generation_loss: 0.70\n",
      "epoch: 10, train acc: 42.00%, classification_loss: 0.27, discrimination_loss: 0.67, generation_loss: 0.70\n",
      "epoch: 11, train acc: 48.00%, classification_loss: 0.27, discrimination_loss: 0.77, generation_loss: 0.56\n",
      "epoch: 12, train acc: 53.00%, classification_loss: 0.27, discrimination_loss: 0.69, generation_loss: 0.61\n",
      "epoch: 13, train acc: 50.00%, classification_loss: 0.26, discrimination_loss: 0.66, generation_loss: 0.70\n",
      "epoch: 14, train acc: 47.00%, classification_loss: 0.26, discrimination_loss: 0.58, generation_loss: 0.86\n",
      "epoch: 15, train acc: 51.00%, classification_loss: 0.25, discrimination_loss: 0.53, generation_loss: 1.05\n",
      "epoch: 16, train acc: 55.00%, classification_loss: 0.23, discrimination_loss: 0.44, generation_loss: 1.08\n",
      "epoch: 17, train acc: 61.00%, classification_loss: 0.25, discrimination_loss: 0.48, generation_loss: 0.94\n",
      "epoch: 18, train acc: 60.00%, classification_loss: 0.25, discrimination_loss: 0.64, generation_loss: 0.69\n",
      "epoch: 19, train acc: 62.00%, classification_loss: 0.25, discrimination_loss: 0.70, generation_loss: 0.69\n",
      "epoch: 20, train acc: 63.00%, classification_loss: 0.25, discrimination_loss: 0.63, generation_loss: 0.84\n",
      "epoch: 21, train acc: 68.00%, classification_loss: 0.23, discrimination_loss: 0.59, generation_loss: 0.92\n",
      "epoch: 22, train acc: 68.00%, classification_loss: 0.25, discrimination_loss: 0.64, generation_loss: 0.75\n",
      "epoch: 23, train acc: 68.00%, classification_loss: 0.27, discrimination_loss: 0.63, generation_loss: 0.74\n",
      "epoch: 24, train acc: 73.00%, classification_loss: 0.23, discrimination_loss: 0.55, generation_loss: 0.88\n",
      "epoch: 25, train acc: 74.00%, classification_loss: 0.22, discrimination_loss: 0.55, generation_loss: 0.80\n",
      "epoch: 26, train acc: 74.00%, classification_loss: 0.22, discrimination_loss: 0.72, generation_loss: 0.58\n",
      "epoch: 27, train acc: 75.00%, classification_loss: 0.21, discrimination_loss: 0.89, generation_loss: 0.44\n",
      "epoch: 28, train acc: 73.00%, classification_loss: 0.24, discrimination_loss: 0.77, generation_loss: 0.55\n",
      "epoch: 29, train acc: 74.00%, classification_loss: 0.21, discrimination_loss: 0.67, generation_loss: 0.75\n",
      "epoch: 30, train acc: 74.00%, classification_loss: 0.21, discrimination_loss: 0.58, generation_loss: 0.92\n",
      "epoch: 31, train acc: 75.00%, classification_loss: 0.19, discrimination_loss: 0.47, generation_loss: 0.96\n",
      "epoch: 32, train acc: 76.00%, classification_loss: 0.21, discrimination_loss: 0.57, generation_loss: 0.88\n",
      "epoch: 33, train acc: 76.00%, classification_loss: 0.20, discrimination_loss: 0.47, generation_loss: 1.05\n",
      "epoch: 34, train acc: 80.00%, classification_loss: 0.23, discrimination_loss: 0.43, generation_loss: 1.12\n",
      "epoch: 35, train acc: 83.00%, classification_loss: 0.20, discrimination_loss: 0.50, generation_loss: 0.94\n",
      "epoch: 36, train acc: 83.00%, classification_loss: 0.19, discrimination_loss: 0.69, generation_loss: 0.57\n",
      "epoch: 37, train acc: 81.00%, classification_loss: 0.20, discrimination_loss: 0.77, generation_loss: 0.53\n",
      "epoch: 38, train acc: 84.00%, classification_loss: 0.23, discrimination_loss: 0.76, generation_loss: 0.58\n",
      "epoch: 39, train acc: 84.00%, classification_loss: 0.18, discrimination_loss: 0.75, generation_loss: 0.65\n",
      "epoch: 40, train acc: 85.00%, classification_loss: 0.18, discrimination_loss: 0.73, generation_loss: 0.73\n",
      "epoch: 41, train acc: 80.00%, classification_loss: 0.17, discrimination_loss: 0.65, generation_loss: 0.78\n",
      "epoch: 42, train acc: 83.00%, classification_loss: 0.19, discrimination_loss: 0.60, generation_loss: 0.86\n",
      "epoch: 43, train acc: 83.00%, classification_loss: 0.20, discrimination_loss: 0.60, generation_loss: 0.91\n",
      "epoch: 44, train acc: 87.00%, classification_loss: 0.18, discrimination_loss: 0.56, generation_loss: 0.97\n",
      "epoch: 45, train acc: 88.00%, classification_loss: 0.19, discrimination_loss: 0.54, generation_loss: 1.11\n",
      "epoch: 46, train acc: 88.00%, classification_loss: 0.15, discrimination_loss: 0.46, generation_loss: 1.20\n",
      "epoch: 47, train acc: 88.00%, classification_loss: 0.16, discrimination_loss: 0.48, generation_loss: 1.03\n",
      "epoch: 48, train acc: 88.00%, classification_loss: 0.15, discrimination_loss: 0.58, generation_loss: 0.84\n",
      "epoch: 49, train acc: 90.00%, classification_loss: 0.15, discrimination_loss: 0.72, generation_loss: 0.48\n",
      "epoch: 50, train acc: 89.00%, classification_loss: 0.17, discrimination_loss: 0.75, generation_loss: 0.46\n",
      "epoch: 51, train acc: 90.00%, classification_loss: 0.12, discrimination_loss: 0.73, generation_loss: 0.51\n",
      "epoch: 52, train acc: 91.00%, classification_loss: 0.16, discrimination_loss: 0.69, generation_loss: 0.59\n",
      "epoch: 53, train acc: 91.00%, classification_loss: 0.16, discrimination_loss: 0.77, generation_loss: 0.62\n",
      "epoch: 54, train acc: 90.00%, classification_loss: 0.16, discrimination_loss: 0.64, generation_loss: 0.66\n",
      "epoch: 55, train acc: 91.00%, classification_loss: 0.15, discrimination_loss: 0.66, generation_loss: 0.72\n",
      "epoch: 56, train acc: 92.00%, classification_loss: 0.15, discrimination_loss: 0.61, generation_loss: 0.73\n",
      "epoch: 57, train acc: 92.00%, classification_loss: 0.15, discrimination_loss: 0.63, generation_loss: 0.64\n",
      "epoch: 58, train acc: 90.00%, classification_loss: 0.15, discrimination_loss: 0.61, generation_loss: 0.70\n",
      "epoch: 59, train acc: 91.00%, classification_loss: 0.14, discrimination_loss: 0.66, generation_loss: 0.73\n",
      "epoch: 60, train acc: 92.00%, classification_loss: 0.14, discrimination_loss: 0.58, generation_loss: 0.73\n",
      "epoch: 61, train acc: 92.00%, classification_loss: 0.14, discrimination_loss: 0.67, generation_loss: 0.66\n",
      "epoch: 62, train acc: 90.00%, classification_loss: 0.14, discrimination_loss: 0.64, generation_loss: 0.63\n",
      "epoch: 63, train acc: 91.00%, classification_loss: 0.14, discrimination_loss: 0.66, generation_loss: 0.54\n",
      "epoch: 64, train acc: 91.00%, classification_loss: 0.14, discrimination_loss: 0.76, generation_loss: 0.57\n",
      "epoch: 65, train acc: 88.00%, classification_loss: 0.14, discrimination_loss: 0.65, generation_loss: 0.62\n",
      "epoch: 66, train acc: 91.00%, classification_loss: 0.15, discrimination_loss: 0.75, generation_loss: 0.58\n",
      "epoch: 67, train acc: 91.00%, classification_loss: 0.15, discrimination_loss: 0.72, generation_loss: 0.63\n",
      "epoch: 68, train acc: 89.00%, classification_loss: 0.15, discrimination_loss: 0.66, generation_loss: 0.62\n",
      "epoch: 69, train acc: 89.00%, classification_loss: 0.13, discrimination_loss: 0.69, generation_loss: 0.60\n",
      "epoch: 70, train acc: 92.00%, classification_loss: 0.17, discrimination_loss: 0.68, generation_loss: 0.66\n",
      "epoch: 71, train acc: 91.00%, classification_loss: 0.17, discrimination_loss: 0.72, generation_loss: 0.65\n",
      "epoch: 72, train acc: 89.00%, classification_loss: 0.17, discrimination_loss: 0.65, generation_loss: 0.66\n",
      "epoch: 73, train acc: 92.00%, classification_loss: 0.17, discrimination_loss: 0.65, generation_loss: 0.62\n",
      "epoch: 74, train acc: 92.00%, classification_loss: 0.17, discrimination_loss: 0.70, generation_loss: 0.59\n",
      "epoch: 75, train acc: 92.00%, classification_loss: 0.17, discrimination_loss: 0.71, generation_loss: 0.67\n",
      "epoch: 76, train acc: 94.00%, classification_loss: 0.17, discrimination_loss: 0.79, generation_loss: 0.61\n",
      "epoch: 77, train acc: 93.00%, classification_loss: 0.17, discrimination_loss: 0.77, generation_loss: 0.61\n",
      "epoch: 78, train acc: 92.00%, classification_loss: 0.17, discrimination_loss: 0.69, generation_loss: 0.61\n",
      "epoch: 79, train acc: 92.00%, classification_loss: 0.17, discrimination_loss: 0.70, generation_loss: 0.62\n",
      "epoch: 80, train acc: 92.00%, classification_loss: 0.17, discrimination_loss: 0.72, generation_loss: 0.63\n",
      "epoch: 81, train acc: 93.00%, classification_loss: 0.17, discrimination_loss: 0.74, generation_loss: 0.61\n",
      "epoch: 82, train acc: 94.00%, classification_loss: 0.17, discrimination_loss: 0.75, generation_loss: 0.60\n",
      "epoch: 83, train acc: 95.00%, classification_loss: 0.17, discrimination_loss: 0.70, generation_loss: 0.62\n",
      "epoch: 84, train acc: 95.00%, classification_loss: 0.17, discrimination_loss: 0.71, generation_loss: 0.62\n",
      "epoch: 85, train acc: 95.00%, classification_loss: 0.17, discrimination_loss: 0.70, generation_loss: 0.63\n",
      "epoch: 86, train acc: 96.00%, classification_loss: 0.17, discrimination_loss: 0.71, generation_loss: 0.61\n",
      "epoch: 87, train acc: 95.00%, classification_loss: 0.17, discrimination_loss: 0.69, generation_loss: 0.60\n",
      "epoch: 88, train acc: 92.00%, classification_loss: 0.17, discrimination_loss: 0.73, generation_loss: 0.63\n",
      "epoch: 89, train acc: 90.00%, classification_loss: 0.17, discrimination_loss: 0.75, generation_loss: 0.66\n",
      "epoch: 90, train acc: 89.00%, classification_loss: 0.15, discrimination_loss: 0.69, generation_loss: 0.62\n",
      "epoch: 91, train acc: 93.00%, classification_loss: 0.19, discrimination_loss: 0.71, generation_loss: 0.63\n",
      "epoch: 92, train acc: 91.00%, classification_loss: 0.19, discrimination_loss: 0.69, generation_loss: 0.66\n",
      "epoch: 93, train acc: 88.00%, classification_loss: 0.19, discrimination_loss: 0.71, generation_loss: 0.68\n",
      "epoch: 94, train acc: 89.00%, classification_loss: 0.19, discrimination_loss: 0.70, generation_loss: 0.68\n",
      "epoch: 95, train acc: 92.00%, classification_loss: 0.16, discrimination_loss: 0.77, generation_loss: 0.70\n",
      "epoch: 96, train acc: 93.00%, classification_loss: 0.16, discrimination_loss: 0.72, generation_loss: 0.69\n",
      "epoch: 97, train acc: 94.00%, classification_loss: 0.16, discrimination_loss: 0.68, generation_loss: 0.73\n",
      "epoch: 98, train acc: 93.00%, classification_loss: 0.16, discrimination_loss: 0.67, generation_loss: 0.73\n",
      "epoch: 99, train acc: 91.00%, classification_loss: 0.16, discrimination_loss: 0.74, generation_loss: 0.71\n",
      "epoch: 100, train acc: 90.00%, classification_loss: 0.16, discrimination_loss: 0.71, generation_loss: 0.69\n",
      "epoch: 101, train acc: 91.00%, classification_loss: 0.18, discrimination_loss: 0.72, generation_loss: 0.76\n",
      "epoch: 102, train acc: 91.00%, classification_loss: 0.18, discrimination_loss: 0.72, generation_loss: 0.73\n",
      "epoch: 103, train acc: 89.00%, classification_loss: 0.18, discrimination_loss: 0.67, generation_loss: 0.73\n",
      "epoch: 104, train acc: 90.00%, classification_loss: 0.17, discrimination_loss: 0.72, generation_loss: 0.71\n",
      "epoch: 105, train acc: 91.00%, classification_loss: 0.17, discrimination_loss: 0.75, generation_loss: 0.72\n",
      "epoch: 106, train acc: 89.00%, classification_loss: 0.17, discrimination_loss: 0.68, generation_loss: 0.75\n",
      "epoch: 107, train acc: 93.00%, classification_loss: 0.18, discrimination_loss: 0.71, generation_loss: 0.77\n",
      "epoch: 108, train acc: 95.00%, classification_loss: 0.18, discrimination_loss: 0.81, generation_loss: 0.78\n",
      "epoch: 109, train acc: 93.00%, classification_loss: 0.18, discrimination_loss: 0.76, generation_loss: 0.81\n",
      "epoch: 110, train acc: 93.00%, classification_loss: 0.18, discrimination_loss: 0.75, generation_loss: 0.82\n",
      "epoch: 111, train acc: 93.00%, classification_loss: 0.18, discrimination_loss: 0.72, generation_loss: 0.80\n",
      "epoch: 112, train acc: 93.00%, classification_loss: 0.18, discrimination_loss: 0.70, generation_loss: 0.81\n",
      "epoch: 113, train acc: 90.00%, classification_loss: 0.18, discrimination_loss: 0.68, generation_loss: 0.82\n",
      "epoch: 114, train acc: 91.00%, classification_loss: 0.18, discrimination_loss: 0.73, generation_loss: 0.81\n",
      "epoch: 115, train acc: 91.00%, classification_loss: 0.18, discrimination_loss: 0.71, generation_loss: 0.81\n",
      "epoch: 116, train acc: 94.00%, classification_loss: 0.18, discrimination_loss: 0.75, generation_loss: 0.81\n",
      "epoch: 117, train acc: 94.00%, classification_loss: 0.18, discrimination_loss: 0.74, generation_loss: 0.83\n",
      "epoch: 118, train acc: 93.00%, classification_loss: 0.18, discrimination_loss: 0.77, generation_loss: 0.85\n",
      "epoch: 119, train acc: 90.00%, classification_loss: 0.18, discrimination_loss: 0.78, generation_loss: 0.86\n",
      "epoch: 120, train acc: 93.00%, classification_loss: 0.22, discrimination_loss: 0.70, generation_loss: 0.83\n",
      "epoch: 121, train acc: 95.00%, classification_loss: 0.22, discrimination_loss: 0.69, generation_loss: 0.86\n",
      "epoch: 122, train acc: 95.00%, classification_loss: 0.22, discrimination_loss: 0.78, generation_loss: 0.87\n",
      "epoch: 123, train acc: 94.00%, classification_loss: 0.22, discrimination_loss: 0.84, generation_loss: 0.87\n",
      "epoch: 124, train acc: 93.00%, classification_loss: 0.22, discrimination_loss: 0.71, generation_loss: 0.92\n",
      "epoch: 125, train acc: 92.00%, classification_loss: 0.22, discrimination_loss: 0.84, generation_loss: 0.92\n",
      "epoch: 126, train acc: 91.00%, classification_loss: 0.22, discrimination_loss: 0.77, generation_loss: 0.93\n",
      "epoch: 127, train acc: 86.00%, classification_loss: 0.22, discrimination_loss: 0.81, generation_loss: 0.97\n",
      "epoch: 128, train acc: 83.00%, classification_loss: 0.19, discrimination_loss: 0.74, generation_loss: 0.93\n",
      "epoch: 129, train acc: 87.00%, classification_loss: 0.18, discrimination_loss: 0.75, generation_loss: 0.96\n",
      "epoch: 130, train acc: 92.00%, classification_loss: 0.21, discrimination_loss: 0.76, generation_loss: 0.98\n",
      "epoch: 131, train acc: 91.00%, classification_loss: 0.21, discrimination_loss: 0.72, generation_loss: 0.97\n",
      "epoch: 132, train acc: 90.00%, classification_loss: 0.21, discrimination_loss: 0.78, generation_loss: 0.99\n",
      "epoch: 133, train acc: 91.00%, classification_loss: 0.18, discrimination_loss: 0.78, generation_loss: 1.02\n",
      "epoch: 134, train acc: 91.00%, classification_loss: 0.18, discrimination_loss: 0.75, generation_loss: 1.03\n",
      "epoch: 135, train acc: 91.00%, classification_loss: 0.18, discrimination_loss: 0.84, generation_loss: 1.04\n",
      "epoch: 136, train acc: 90.00%, classification_loss: 0.18, discrimination_loss: 0.81, generation_loss: 1.01\n",
      "epoch: 137, train acc: 91.00%, classification_loss: 0.17, discrimination_loss: 0.83, generation_loss: 1.02\n",
      "epoch: 138, train acc: 90.00%, classification_loss: 0.17, discrimination_loss: 0.74, generation_loss: 1.06\n",
      "epoch: 139, train acc: 91.00%, classification_loss: 0.18, discrimination_loss: 0.82, generation_loss: 1.06\n",
      "epoch: 140, train acc: 92.00%, classification_loss: 0.18, discrimination_loss: 0.84, generation_loss: 1.06\n",
      "epoch: 141, train acc: 92.00%, classification_loss: 0.18, discrimination_loss: 0.83, generation_loss: 1.08\n",
      "epoch: 142, train acc: 92.00%, classification_loss: 0.18, discrimination_loss: 0.84, generation_loss: 1.11\n",
      "epoch: 143, train acc: 91.00%, classification_loss: 0.18, discrimination_loss: 0.87, generation_loss: 1.13\n",
      "epoch: 144, train acc: 90.00%, classification_loss: 0.18, discrimination_loss: 0.78, generation_loss: 1.16\n",
      "epoch: 145, train acc: 91.00%, classification_loss: 0.17, discrimination_loss: 0.80, generation_loss: 1.16\n",
      "epoch: 146, train acc: 91.00%, classification_loss: 0.17, discrimination_loss: 0.78, generation_loss: 1.19\n",
      "epoch: 147, train acc: 92.00%, classification_loss: 0.17, discrimination_loss: 0.82, generation_loss: 1.18\n",
      "epoch: 148, train acc: 90.00%, classification_loss: 0.17, discrimination_loss: 0.84, generation_loss: 1.18\n",
      "epoch: 149, train acc: 91.00%, classification_loss: 0.17, discrimination_loss: 0.80, generation_loss: 1.19\n",
      "epoch: 150, train acc: 88.00%, classification_loss: 0.17, discrimination_loss: 0.83, generation_loss: 1.21\n",
      "epoch: 151, train acc: 91.00%, classification_loss: 0.17, discrimination_loss: 0.84, generation_loss: 1.23\n",
      "epoch: 152, train acc: 91.00%, classification_loss: 0.17, discrimination_loss: 0.82, generation_loss: 1.24\n",
      "epoch: 153, train acc: 92.00%, classification_loss: 0.17, discrimination_loss: 0.85, generation_loss: 1.26\n",
      "epoch: 154, train acc: 92.00%, classification_loss: 0.17, discrimination_loss: 0.90, generation_loss: 1.28\n",
      "epoch: 155, train acc: 92.00%, classification_loss: 0.17, discrimination_loss: 0.87, generation_loss: 1.30\n",
      "epoch: 156, train acc: 91.00%, classification_loss: 0.17, discrimination_loss: 0.92, generation_loss: 1.33\n",
      "epoch: 157, train acc: 91.00%, classification_loss: 0.17, discrimination_loss: 0.83, generation_loss: 1.35\n",
      "epoch: 158, train acc: 89.00%, classification_loss: 0.17, discrimination_loss: 0.93, generation_loss: 1.38\n",
      "epoch: 159, train acc: 89.00%, classification_loss: 0.18, discrimination_loss: 0.98, generation_loss: 1.37\n",
      "epoch: 160, train acc: 91.00%, classification_loss: 0.15, discrimination_loss: 0.99, generation_loss: 1.40\n",
      "epoch: 161, train acc: 91.00%, classification_loss: 0.15, discrimination_loss: 0.92, generation_loss: 1.40\n",
      "epoch: 162, train acc: 89.00%, classification_loss: 0.15, discrimination_loss: 0.91, generation_loss: 1.39\n",
      "epoch: 163, train acc: 88.00%, classification_loss: 0.18, discrimination_loss: 0.88, generation_loss: 1.41\n",
      "epoch: 164, train acc: 86.00%, classification_loss: 0.13, discrimination_loss: 0.89, generation_loss: 1.41\n",
      "epoch: 165, train acc: 87.00%, classification_loss: 0.18, discrimination_loss: 0.90, generation_loss: 1.45\n",
      "epoch: 166, train acc: 89.00%, classification_loss: 0.15, discrimination_loss: 0.91, generation_loss: 1.45\n",
      "epoch: 167, train acc: 90.00%, classification_loss: 0.19, discrimination_loss: 0.98, generation_loss: 1.46\n",
      "epoch: 168, train acc: 90.00%, classification_loss: 0.20, discrimination_loss: 0.93, generation_loss: 1.46\n",
      "epoch: 169, train acc: 91.00%, classification_loss: 0.17, discrimination_loss: 0.93, generation_loss: 1.49\n",
      "epoch: 170, train acc: 91.00%, classification_loss: 0.17, discrimination_loss: 1.02, generation_loss: 1.49\n",
      "epoch: 171, train acc: 93.00%, classification_loss: 0.17, discrimination_loss: 1.03, generation_loss: 1.52\n",
      "epoch: 172, train acc: 93.00%, classification_loss: 0.17, discrimination_loss: 0.90, generation_loss: 1.54\n",
      "epoch: 173, train acc: 92.00%, classification_loss: 0.17, discrimination_loss: 0.93, generation_loss: 1.54\n",
      "epoch: 174, train acc: 91.00%, classification_loss: 0.17, discrimination_loss: 0.95, generation_loss: 1.56\n",
      "epoch: 175, train acc: 91.00%, classification_loss: 0.17, discrimination_loss: 0.96, generation_loss: 1.55\n",
      "epoch: 176, train acc: 89.00%, classification_loss: 0.17, discrimination_loss: 0.95, generation_loss: 1.57\n",
      "epoch: 177, train acc: 89.00%, classification_loss: 0.18, discrimination_loss: 0.94, generation_loss: 1.57\n",
      "epoch: 178, train acc: 89.00%, classification_loss: 0.17, discrimination_loss: 0.95, generation_loss: 1.59\n",
      "epoch: 179, train acc: 91.00%, classification_loss: 0.15, discrimination_loss: 0.96, generation_loss: 1.60\n",
      "epoch: 180, train acc: 91.00%, classification_loss: 0.15, discrimination_loss: 0.96, generation_loss: 1.62\n",
      "epoch: 181, train acc: 91.00%, classification_loss: 0.15, discrimination_loss: 1.02, generation_loss: 1.63\n",
      "epoch: 182, train acc: 90.00%, classification_loss: 0.15, discrimination_loss: 1.07, generation_loss: 1.64\n",
      "epoch: 183, train acc: 90.00%, classification_loss: 0.16, discrimination_loss: 1.01, generation_loss: 1.65\n",
      "epoch: 184, train acc: 88.00%, classification_loss: 0.16, discrimination_loss: 0.96, generation_loss: 1.68\n",
      "epoch: 185, train acc: 90.00%, classification_loss: 0.16, discrimination_loss: 0.97, generation_loss: 1.68\n",
      "epoch: 186, train acc: 92.00%, classification_loss: 0.17, discrimination_loss: 0.94, generation_loss: 1.72\n",
      "epoch: 187, train acc: 93.00%, classification_loss: 0.17, discrimination_loss: 0.98, generation_loss: 1.73\n",
      "epoch: 188, train acc: 92.00%, classification_loss: 0.17, discrimination_loss: 0.98, generation_loss: 1.73\n",
      "epoch: 189, train acc: 90.00%, classification_loss: 0.17, discrimination_loss: 0.99, generation_loss: 1.73\n",
      "epoch: 190, train acc: 90.00%, classification_loss: 0.16, discrimination_loss: 1.03, generation_loss: 1.73\n",
      "epoch: 191, train acc: 90.00%, classification_loss: 0.17, discrimination_loss: 1.02, generation_loss: 1.76\n",
      "epoch: 192, train acc: 90.00%, classification_loss: 0.15, discrimination_loss: 1.03, generation_loss: 1.75\n",
      "epoch: 193, train acc: 91.00%, classification_loss: 0.15, discrimination_loss: 0.97, generation_loss: 1.79\n",
      "epoch: 194, train acc: 91.00%, classification_loss: 0.15, discrimination_loss: 0.99, generation_loss: 1.78\n",
      "epoch: 195, train acc: 91.00%, classification_loss: 0.15, discrimination_loss: 1.12, generation_loss: 1.82\n",
      "epoch: 196, train acc: 92.00%, classification_loss: 0.15, discrimination_loss: 1.06, generation_loss: 1.82\n",
      "epoch: 197, train acc: 92.00%, classification_loss: 0.15, discrimination_loss: 0.99, generation_loss: 1.82\n",
      "epoch: 198, train acc: 93.00%, classification_loss: 0.15, discrimination_loss: 1.11, generation_loss: 1.82\n",
      "epoch: 199, train acc: 92.00%, classification_loss: 0.15, discrimination_loss: 1.04, generation_loss: 1.84\n",
      "epoch: 200, train acc: 91.00%, classification_loss: 0.15, discrimination_loss: 1.07, generation_loss: 1.88\n",
      "epoch: 201, train acc: 90.00%, classification_loss: 0.15, discrimination_loss: 1.03, generation_loss: 1.86\n",
      "epoch: 202, train acc: 91.00%, classification_loss: 0.15, discrimination_loss: 1.06, generation_loss: 1.84\n",
      "epoch: 203, train acc: 91.00%, classification_loss: 0.15, discrimination_loss: 1.09, generation_loss: 1.90\n",
      "epoch: 204, train acc: 91.00%, classification_loss: 0.15, discrimination_loss: 1.05, generation_loss: 1.88\n",
      "epoch: 205, train acc: 93.00%, classification_loss: 0.15, discrimination_loss: 1.10, generation_loss: 1.90\n",
      "epoch: 206, train acc: 92.00%, classification_loss: 0.15, discrimination_loss: 1.08, generation_loss: 1.89\n",
      "epoch: 207, train acc: 92.00%, classification_loss: 0.15, discrimination_loss: 1.10, generation_loss: 1.90\n",
      "epoch: 208, train acc: 92.00%, classification_loss: 0.15, discrimination_loss: 1.01, generation_loss: 1.91\n",
      "epoch: 209, train acc: 92.00%, classification_loss: 0.15, discrimination_loss: 1.07, generation_loss: 1.91\n",
      "epoch: 210, train acc: 90.00%, classification_loss: 0.15, discrimination_loss: 1.11, generation_loss: 1.92\n",
      "epoch: 211, train acc: 92.00%, classification_loss: 0.17, discrimination_loss: 1.06, generation_loss: 1.97\n",
      "epoch: 212, train acc: 92.00%, classification_loss: 0.17, discrimination_loss: 1.07, generation_loss: 1.98\n",
      "epoch: 213, train acc: 91.00%, classification_loss: 0.17, discrimination_loss: 1.11, generation_loss: 1.99\n",
      "epoch: 214, train acc: 92.00%, classification_loss: 0.17, discrimination_loss: 1.24, generation_loss: 1.99\n",
      "epoch: 215, train acc: 92.00%, classification_loss: 0.17, discrimination_loss: 1.10, generation_loss: 1.97\n",
      "epoch: 216, train acc: 91.00%, classification_loss: 0.17, discrimination_loss: 1.10, generation_loss: 2.00\n",
      "epoch: 217, train acc: 91.00%, classification_loss: 0.17, discrimination_loss: 1.13, generation_loss: 2.00\n",
      "epoch: 218, train acc: 90.00%, classification_loss: 0.17, discrimination_loss: 1.11, generation_loss: 2.00\n",
      "epoch: 219, train acc: 91.00%, classification_loss: 0.18, discrimination_loss: 1.15, generation_loss: 2.03\n",
      "epoch: 220, train acc: 91.00%, classification_loss: 0.18, discrimination_loss: 1.09, generation_loss: 2.00\n",
      "epoch: 221, train acc: 91.00%, classification_loss: 0.18, discrimination_loss: 1.13, generation_loss: 2.02\n",
      "epoch: 222, train acc: 91.00%, classification_loss: 0.18, discrimination_loss: 1.11, generation_loss: 2.05\n",
      "epoch: 223, train acc: 91.00%, classification_loss: 0.18, discrimination_loss: 1.15, generation_loss: 2.05\n",
      "epoch: 224, train acc: 91.00%, classification_loss: 0.18, discrimination_loss: 1.13, generation_loss: 2.05\n",
      "epoch: 225, train acc: 92.00%, classification_loss: 0.18, discrimination_loss: 1.09, generation_loss: 2.04\n",
      "epoch: 226, train acc: 88.00%, classification_loss: 0.18, discrimination_loss: 1.16, generation_loss: 2.06\n",
      "epoch: 227, train acc: 89.00%, classification_loss: 0.15, discrimination_loss: 1.14, generation_loss: 2.08\n",
      "epoch: 228, train acc: 89.00%, classification_loss: 0.16, discrimination_loss: 1.16, generation_loss: 2.09\n",
      "epoch: 229, train acc: 89.00%, classification_loss: 0.16, discrimination_loss: 1.16, generation_loss: 2.11\n",
      "epoch: 230, train acc: 91.00%, classification_loss: 0.13, discrimination_loss: 1.12, generation_loss: 2.12\n",
      "epoch: 231, train acc: 91.00%, classification_loss: 0.13, discrimination_loss: 1.16, generation_loss: 2.12\n",
      "epoch: 232, train acc: 91.00%, classification_loss: 0.13, discrimination_loss: 1.16, generation_loss: 2.14\n",
      "epoch: 233, train acc: 90.00%, classification_loss: 0.13, discrimination_loss: 1.16, generation_loss: 2.10\n",
      "epoch: 234, train acc: 90.00%, classification_loss: 0.16, discrimination_loss: 1.18, generation_loss: 2.13\n",
      "epoch: 235, train acc: 90.00%, classification_loss: 0.14, discrimination_loss: 1.15, generation_loss: 2.14\n",
      "epoch: 236, train acc: 90.00%, classification_loss: 0.14, discrimination_loss: 1.17, generation_loss: 2.15\n",
      "epoch: 237, train acc: 90.00%, classification_loss: 0.17, discrimination_loss: 1.20, generation_loss: 2.15\n",
      "epoch: 238, train acc: 90.00%, classification_loss: 0.13, discrimination_loss: 1.14, generation_loss: 2.14\n",
      "epoch: 239, train acc: 91.00%, classification_loss: 0.15, discrimination_loss: 1.17, generation_loss: 2.15\n",
      "epoch: 240, train acc: 91.00%, classification_loss: 0.15, discrimination_loss: 1.19, generation_loss: 2.16\n",
      "epoch: 241, train acc: 92.00%, classification_loss: 0.15, discrimination_loss: 1.15, generation_loss: 2.14\n",
      "epoch: 242, train acc: 91.00%, classification_loss: 0.15, discrimination_loss: 1.17, generation_loss: 2.15\n",
      "epoch: 243, train acc: 90.00%, classification_loss: 0.15, discrimination_loss: 1.23, generation_loss: 2.16\n",
      "epoch: 244, train acc: 91.00%, classification_loss: 0.13, discrimination_loss: 1.18, generation_loss: 2.15\n",
      "epoch: 245, train acc: 91.00%, classification_loss: 0.13, discrimination_loss: 1.14, generation_loss: 2.17\n",
      "epoch: 246, train acc: 92.00%, classification_loss: 0.13, discrimination_loss: 1.25, generation_loss: 2.19\n",
      "epoch: 247, train acc: 92.00%, classification_loss: 0.13, discrimination_loss: 1.20, generation_loss: 2.17\n",
      "epoch: 248, train acc: 92.00%, classification_loss: 0.13, discrimination_loss: 1.22, generation_loss: 2.16\n",
      "epoch: 249, train acc: 90.00%, classification_loss: 0.13, discrimination_loss: 1.20, generation_loss: 2.18\n",
      "epoch: 250, train acc: 90.00%, classification_loss: 0.12, discrimination_loss: 1.18, generation_loss: 2.21\n",
      "epoch: 251, train acc: 90.00%, classification_loss: 0.13, discrimination_loss: 1.19, generation_loss: 2.26\n",
      "epoch: 252, train acc: 92.00%, classification_loss: 0.14, discrimination_loss: 1.18, generation_loss: 2.20\n",
      "epoch: 253, train acc: 91.00%, classification_loss: 0.14, discrimination_loss: 1.18, generation_loss: 2.22\n",
      "epoch: 254, train acc: 91.00%, classification_loss: 0.14, discrimination_loss: 1.23, generation_loss: 2.22\n",
      "epoch: 255, train acc: 92.00%, classification_loss: 0.14, discrimination_loss: 1.24, generation_loss: 2.22\n",
      "epoch: 256, train acc: 92.00%, classification_loss: 0.14, discrimination_loss: 1.21, generation_loss: 2.25\n",
      "epoch: 257, train acc: 92.00%, classification_loss: 0.14, discrimination_loss: 1.28, generation_loss: 2.23\n",
      "epoch: 258, train acc: 92.00%, classification_loss: 0.14, discrimination_loss: 1.28, generation_loss: 2.25\n",
      "epoch: 259, train acc: 92.00%, classification_loss: 0.14, discrimination_loss: 1.25, generation_loss: 2.23\n",
      "epoch: 260, train acc: 93.00%, classification_loss: 0.14, discrimination_loss: 1.21, generation_loss: 2.27\n",
      "epoch: 261, train acc: 93.00%, classification_loss: 0.14, discrimination_loss: 1.18, generation_loss: 2.26\n",
      "epoch: 262, train acc: 92.00%, classification_loss: 0.14, discrimination_loss: 1.26, generation_loss: 2.20\n",
      "epoch: 263, train acc: 92.00%, classification_loss: 0.14, discrimination_loss: 1.24, generation_loss: 2.26\n",
      "epoch: 264, train acc: 92.00%, classification_loss: 0.14, discrimination_loss: 1.22, generation_loss: 2.29\n",
      "epoch: 265, train acc: 91.00%, classification_loss: 0.14, discrimination_loss: 1.29, generation_loss: 2.28\n",
      "epoch: 266, train acc: 92.00%, classification_loss: 0.14, discrimination_loss: 1.23, generation_loss: 2.25\n",
      "epoch: 267, train acc: 92.00%, classification_loss: 0.14, discrimination_loss: 1.31, generation_loss: 2.25\n",
      "epoch: 268, train acc: 92.00%, classification_loss: 0.14, discrimination_loss: 1.25, generation_loss: 2.29\n",
      "epoch: 269, train acc: 92.00%, classification_loss: 0.14, discrimination_loss: 1.23, generation_loss: 2.29\n",
      "epoch: 270, train acc: 93.00%, classification_loss: 0.14, discrimination_loss: 1.23, generation_loss: 2.29\n",
      "epoch: 271, train acc: 93.00%, classification_loss: 0.14, discrimination_loss: 1.25, generation_loss: 2.27\n",
      "epoch: 272, train acc: 93.00%, classification_loss: 0.14, discrimination_loss: 1.27, generation_loss: 2.34\n",
      "epoch: 273, train acc: 91.00%, classification_loss: 0.14, discrimination_loss: 1.26, generation_loss: 2.26\n",
      "epoch: 274, train acc: 90.00%, classification_loss: 0.14, discrimination_loss: 1.28, generation_loss: 2.29\n",
      "epoch: 275, train acc: 90.00%, classification_loss: 0.14, discrimination_loss: 1.26, generation_loss: 2.30\n",
      "epoch: 276, train acc: 91.00%, classification_loss: 0.18, discrimination_loss: 1.27, generation_loss: 2.27\n",
      "epoch: 277, train acc: 93.00%, classification_loss: 0.18, discrimination_loss: 1.26, generation_loss: 2.25\n",
      "epoch: 278, train acc: 93.00%, classification_loss: 0.18, discrimination_loss: 1.29, generation_loss: 2.33\n",
      "epoch: 279, train acc: 92.00%, classification_loss: 0.18, discrimination_loss: 1.26, generation_loss: 2.31\n",
      "epoch: 280, train acc: 91.00%, classification_loss: 0.18, discrimination_loss: 1.27, generation_loss: 2.32\n",
      "epoch: 281, train acc: 92.00%, classification_loss: 0.18, discrimination_loss: 1.24, generation_loss: 2.34\n",
      "epoch: 282, train acc: 92.00%, classification_loss: 0.18, discrimination_loss: 1.25, generation_loss: 2.36\n",
      "epoch: 283, train acc: 92.00%, classification_loss: 0.18, discrimination_loss: 1.28, generation_loss: 2.29\n",
      "epoch: 284, train acc: 91.00%, classification_loss: 0.18, discrimination_loss: 1.30, generation_loss: 2.34\n",
      "epoch: 285, train acc: 87.00%, classification_loss: 0.18, discrimination_loss: 1.31, generation_loss: 2.30\n",
      "epoch: 286, train acc: 92.00%, classification_loss: 0.16, discrimination_loss: 1.24, generation_loss: 2.34\n",
      "epoch: 287, train acc: 91.00%, classification_loss: 0.16, discrimination_loss: 1.24, generation_loss: 2.31\n",
      "epoch: 288, train acc: 90.00%, classification_loss: 0.16, discrimination_loss: 1.29, generation_loss: 2.30\n",
      "epoch: 289, train acc: 92.00%, classification_loss: 0.16, discrimination_loss: 1.30, generation_loss: 2.34\n",
      "epoch: 290, train acc: 95.00%, classification_loss: 0.16, discrimination_loss: 1.27, generation_loss: 2.38\n",
      "epoch: 291, train acc: 96.00%, classification_loss: 0.16, discrimination_loss: 1.31, generation_loss: 2.34\n",
      "epoch: 292, train acc: 96.00%, classification_loss: 0.16, discrimination_loss: 1.28, generation_loss: 2.48\n",
      "epoch: 293, train acc: 96.00%, classification_loss: 0.16, discrimination_loss: 1.26, generation_loss: 2.28\n",
      "epoch: 294, train acc: 96.00%, classification_loss: 0.16, discrimination_loss: 1.31, generation_loss: 2.34\n",
      "epoch: 295, train acc: 96.00%, classification_loss: 0.16, discrimination_loss: 1.37, generation_loss: 2.36\n",
      "epoch: 296, train acc: 96.00%, classification_loss: 0.16, discrimination_loss: 1.25, generation_loss: 2.40\n",
      "epoch: 297, train acc: 93.00%, classification_loss: 0.16, discrimination_loss: 1.34, generation_loss: 2.34\n",
      "epoch: 298, train acc: 93.00%, classification_loss: 0.16, discrimination_loss: 1.29, generation_loss: 2.39\n",
      "epoch: 299, train acc: 93.00%, classification_loss: 0.16, discrimination_loss: 1.32, generation_loss: 2.28\n",
      "epoch: 300, train acc: 93.00%, classification_loss: 0.16, discrimination_loss: 1.30, generation_loss: 2.34\n",
      "epoch: 301, train acc: 91.00%, classification_loss: 0.16, discrimination_loss: 1.26, generation_loss: 2.29\n",
      "epoch: 302, train acc: 91.00%, classification_loss: 0.16, discrimination_loss: 1.35, generation_loss: 2.34\n",
      "epoch: 303, train acc: 89.00%, classification_loss: 0.16, discrimination_loss: 1.25, generation_loss: 2.35\n",
      "epoch: 304, train acc: 89.00%, classification_loss: 0.17, discrimination_loss: 1.30, generation_loss: 2.31\n",
      "epoch: 305, train acc: 90.00%, classification_loss: 0.12, discrimination_loss: 1.30, generation_loss: 2.34\n",
      "epoch: 306, train acc: 90.00%, classification_loss: 0.14, discrimination_loss: 1.30, generation_loss: 2.27\n",
      "epoch: 307, train acc: 91.00%, classification_loss: 0.10, discrimination_loss: 1.34, generation_loss: 2.32\n",
      "epoch: 308, train acc: 91.00%, classification_loss: 0.10, discrimination_loss: 1.31, generation_loss: 2.30\n",
      "epoch: 309, train acc: 90.00%, classification_loss: 0.10, discrimination_loss: 1.34, generation_loss: 2.36\n",
      "epoch: 310, train acc: 90.00%, classification_loss: 0.12, discrimination_loss: 1.29, generation_loss: 2.30\n",
      "epoch: 311, train acc: 91.00%, classification_loss: 0.15, discrimination_loss: 1.33, generation_loss: 2.38\n",
      "epoch: 312, train acc: 91.00%, classification_loss: 0.15, discrimination_loss: 1.35, generation_loss: 2.34\n",
      "epoch: 313, train acc: 92.00%, classification_loss: 0.15, discrimination_loss: 1.30, generation_loss: 2.30\n",
      "epoch: 314, train acc: 92.00%, classification_loss: 0.15, discrimination_loss: 1.37, generation_loss: 2.36\n",
      "epoch: 315, train acc: 92.00%, classification_loss: 0.15, discrimination_loss: 1.31, generation_loss: 2.29\n",
      "epoch: 316, train acc: 91.00%, classification_loss: 0.15, discrimination_loss: 1.32, generation_loss: 2.33\n",
      "epoch: 317, train acc: 91.00%, classification_loss: 0.15, discrimination_loss: 1.33, generation_loss: 2.33\n",
      "epoch: 318, train acc: 90.00%, classification_loss: 0.15, discrimination_loss: 1.40, generation_loss: 2.36\n",
      "epoch: 319, train acc: 90.00%, classification_loss: 0.12, discrimination_loss: 1.33, generation_loss: 2.35\n",
      "epoch: 320, train acc: 90.00%, classification_loss: 0.12, discrimination_loss: 1.30, generation_loss: 2.43\n",
      "epoch: 321, train acc: 92.00%, classification_loss: 0.14, discrimination_loss: 1.33, generation_loss: 2.39\n",
      "epoch: 322, train acc: 92.00%, classification_loss: 0.14, discrimination_loss: 1.30, generation_loss: 2.35\n",
      "epoch: 323, train acc: 93.00%, classification_loss: 0.14, discrimination_loss: 1.33, generation_loss: 2.35\n",
      "epoch: 324, train acc: 94.00%, classification_loss: 0.14, discrimination_loss: 1.27, generation_loss: 2.28\n",
      "epoch: 325, train acc: 93.00%, classification_loss: 0.14, discrimination_loss: 1.31, generation_loss: 2.42\n",
      "epoch: 326, train acc: 91.00%, classification_loss: 0.14, discrimination_loss: 1.37, generation_loss: 2.33\n",
      "epoch: 327, train acc: 90.00%, classification_loss: 0.14, discrimination_loss: 1.39, generation_loss: 2.38\n",
      "epoch: 328, train acc: 92.00%, classification_loss: 0.13, discrimination_loss: 1.39, generation_loss: 2.38\n",
      "epoch: 329, train acc: 91.00%, classification_loss: 0.13, discrimination_loss: 1.40, generation_loss: 2.30\n",
      "epoch: 330, train acc: 92.00%, classification_loss: 0.13, discrimination_loss: 1.31, generation_loss: 2.25\n",
      "epoch: 331, train acc: 91.00%, classification_loss: 0.13, discrimination_loss: 1.34, generation_loss: 2.43\n",
      "epoch: 332, train acc: 91.00%, classification_loss: 0.13, discrimination_loss: 1.38, generation_loss: 2.24\n",
      "epoch: 333, train acc: 91.00%, classification_loss: 0.13, discrimination_loss: 1.36, generation_loss: 2.32\n",
      "epoch: 334, train acc: 91.00%, classification_loss: 0.13, discrimination_loss: 1.42, generation_loss: 2.30\n",
      "epoch: 335, train acc: 90.00%, classification_loss: 0.13, discrimination_loss: 1.30, generation_loss: 2.50\n",
      "epoch: 336, train acc: 90.00%, classification_loss: 0.14, discrimination_loss: 1.39, generation_loss: 2.33\n",
      "epoch: 337, train acc: 94.00%, classification_loss: 0.12, discrimination_loss: 1.35, generation_loss: 2.35\n",
      "epoch: 338, train acc: 94.00%, classification_loss: 0.12, discrimination_loss: 1.35, generation_loss: 2.20\n",
      "epoch: 339, train acc: 94.00%, classification_loss: 0.12, discrimination_loss: 1.34, generation_loss: 2.31\n",
      "epoch: 340, train acc: 93.00%, classification_loss: 0.12, discrimination_loss: 1.33, generation_loss: 2.31\n",
      "epoch: 341, train acc: 93.00%, classification_loss: 0.12, discrimination_loss: 1.41, generation_loss: 2.40\n",
      "epoch: 342, train acc: 93.00%, classification_loss: 0.12, discrimination_loss: 1.34, generation_loss: 2.27\n",
      "epoch: 343, train acc: 93.00%, classification_loss: 0.12, discrimination_loss: 1.39, generation_loss: 2.33\n",
      "epoch: 344, train acc: 94.00%, classification_loss: 0.12, discrimination_loss: 1.31, generation_loss: 2.23\n",
      "epoch: 345, train acc: 93.00%, classification_loss: 0.12, discrimination_loss: 1.35, generation_loss: 2.28\n",
      "epoch: 346, train acc: 93.00%, classification_loss: 0.12, discrimination_loss: 1.36, generation_loss: 2.35\n",
      "epoch: 347, train acc: 93.00%, classification_loss: 0.12, discrimination_loss: 1.30, generation_loss: 2.25\n",
      "epoch: 348, train acc: 91.00%, classification_loss: 0.12, discrimination_loss: 1.35, generation_loss: 2.27\n",
      "epoch: 349, train acc: 90.00%, classification_loss: 0.12, discrimination_loss: 1.34, generation_loss: 2.27\n",
      "epoch: 350, train acc: 90.00%, classification_loss: 0.10, discrimination_loss: 1.36, generation_loss: 2.27\n",
      "epoch: 351, train acc: 91.00%, classification_loss: 0.16, discrimination_loss: 1.37, generation_loss: 2.21\n",
      "epoch: 352, train acc: 91.00%, classification_loss: 0.16, discrimination_loss: 1.33, generation_loss: 2.05\n",
      "epoch: 353, train acc: 92.00%, classification_loss: 0.16, discrimination_loss: 1.35, generation_loss: 2.30\n",
      "epoch: 354, train acc: 92.00%, classification_loss: 0.16, discrimination_loss: 1.33, generation_loss: 2.29\n",
      "epoch: 355, train acc: 93.00%, classification_loss: 0.16, discrimination_loss: 1.38, generation_loss: 2.27\n",
      "epoch: 356, train acc: 93.00%, classification_loss: 0.16, discrimination_loss: 1.41, generation_loss: 2.26\n",
      "epoch: 357, train acc: 95.00%, classification_loss: 0.16, discrimination_loss: 1.39, generation_loss: 2.24\n",
      "epoch: 358, train acc: 93.00%, classification_loss: 0.16, discrimination_loss: 1.36, generation_loss: 2.02\n",
      "epoch: 359, train acc: 91.00%, classification_loss: 0.16, discrimination_loss: 1.38, generation_loss: 2.34\n",
      "epoch: 360, train acc: 90.00%, classification_loss: 0.16, discrimination_loss: 1.35, generation_loss: 2.17\n",
      "epoch: 361, train acc: 91.00%, classification_loss: 0.14, discrimination_loss: 1.37, generation_loss: 2.23\n",
      "epoch: 362, train acc: 91.00%, classification_loss: 0.14, discrimination_loss: 1.31, generation_loss: 2.13\n",
      "epoch: 363, train acc: 89.00%, classification_loss: 0.14, discrimination_loss: 1.40, generation_loss: 2.45\n",
      "epoch: 364, train acc: 92.00%, classification_loss: 0.13, discrimination_loss: 1.34, generation_loss: 2.07\n",
      "epoch: 365, train acc: 92.00%, classification_loss: 0.13, discrimination_loss: 1.31, generation_loss: 2.36\n",
      "epoch: 366, train acc: 91.00%, classification_loss: 0.13, discrimination_loss: 1.37, generation_loss: 2.31\n",
      "epoch: 367, train acc: 89.00%, classification_loss: 0.13, discrimination_loss: 1.30, generation_loss: 2.33\n",
      "epoch: 368, train acc: 92.00%, classification_loss: 0.13, discrimination_loss: 1.44, generation_loss: 2.39\n",
      "epoch: 369, train acc: 92.00%, classification_loss: 0.13, discrimination_loss: 1.42, generation_loss: 2.20\n",
      "epoch: 370, train acc: 92.00%, classification_loss: 0.13, discrimination_loss: 1.43, generation_loss: 2.06\n",
      "epoch: 371, train acc: 92.00%, classification_loss: 0.13, discrimination_loss: 1.35, generation_loss: 2.29\n",
      "epoch: 372, train acc: 92.00%, classification_loss: 0.13, discrimination_loss: 1.38, generation_loss: 2.27\n",
      "epoch: 373, train acc: 90.00%, classification_loss: 0.13, discrimination_loss: 1.48, generation_loss: 2.12\n",
      "epoch: 374, train acc: 92.00%, classification_loss: 0.15, discrimination_loss: 1.36, generation_loss: 1.94\n",
      "epoch: 375, train acc: 92.00%, classification_loss: 0.15, discrimination_loss: 1.35, generation_loss: 2.39\n",
      "epoch: 376, train acc: 90.00%, classification_loss: 0.15, discrimination_loss: 1.33, generation_loss: 2.09\n",
      "epoch: 377, train acc: 94.00%, classification_loss: 0.11, discrimination_loss: 1.35, generation_loss: 2.10\n",
      "epoch: 378, train acc: 92.00%, classification_loss: 0.11, discrimination_loss: 1.39, generation_loss: 2.00\n",
      "epoch: 379, train acc: 91.00%, classification_loss: 0.11, discrimination_loss: 1.37, generation_loss: 2.14\n",
      "epoch: 380, train acc: 89.00%, classification_loss: 0.11, discrimination_loss: 1.41, generation_loss: 2.24\n",
      "epoch: 381, train acc: 92.00%, classification_loss: 0.12, discrimination_loss: 1.33, generation_loss: 2.17\n",
      "epoch: 382, train acc: 92.00%, classification_loss: 0.12, discrimination_loss: 1.40, generation_loss: 2.16\n",
      "epoch: 383, train acc: 92.00%, classification_loss: 0.12, discrimination_loss: 1.46, generation_loss: 2.02\n",
      "epoch: 384, train acc: 92.00%, classification_loss: 0.12, discrimination_loss: 1.46, generation_loss: 2.41\n",
      "epoch: 385, train acc: 90.00%, classification_loss: 0.12, discrimination_loss: 1.37, generation_loss: 2.22\n",
      "epoch: 386, train acc: 94.00%, classification_loss: 0.13, discrimination_loss: 1.48, generation_loss: 2.08\n",
      "epoch: 387, train acc: 94.00%, classification_loss: 0.13, discrimination_loss: 1.35, generation_loss: 2.10\n",
      "epoch: 388, train acc: 93.00%, classification_loss: 0.13, discrimination_loss: 1.36, generation_loss: 1.96\n",
      "epoch: 389, train acc: 93.00%, classification_loss: 0.13, discrimination_loss: 1.44, generation_loss: 1.91\n",
      "epoch: 390, train acc: 93.00%, classification_loss: 0.13, discrimination_loss: 1.44, generation_loss: 2.09\n",
      "epoch: 391, train acc: 92.00%, classification_loss: 0.13, discrimination_loss: 1.34, generation_loss: 2.00\n",
      "epoch: 392, train acc: 91.00%, classification_loss: 0.13, discrimination_loss: 1.40, generation_loss: 1.78\n",
      "epoch: 393, train acc: 89.00%, classification_loss: 0.13, discrimination_loss: 1.44, generation_loss: 1.95\n",
      "epoch: 394, train acc: 93.00%, classification_loss: 0.13, discrimination_loss: 1.32, generation_loss: 2.07\n",
      "epoch: 395, train acc: 93.00%, classification_loss: 0.13, discrimination_loss: 1.41, generation_loss: 2.29\n",
      "epoch: 396, train acc: 93.00%, classification_loss: 0.13, discrimination_loss: 1.39, generation_loss: 2.34\n",
      "epoch: 397, train acc: 93.00%, classification_loss: 0.13, discrimination_loss: 1.41, generation_loss: 2.20\n",
      "epoch: 398, train acc: 93.00%, classification_loss: 0.13, discrimination_loss: 1.30, generation_loss: 2.13\n",
      "epoch: 399, train acc: 92.00%, classification_loss: 0.13, discrimination_loss: 1.32, generation_loss: 2.05\n",
      "epoch: 400, train acc: 92.00%, classification_loss: 0.13, discrimination_loss: 1.40, generation_loss: 1.98\n",
      "epoch: 401, train acc: 90.00%, classification_loss: 0.13, discrimination_loss: 1.40, generation_loss: 2.04\n",
      "epoch: 402, train acc: 91.00%, classification_loss: 0.12, discrimination_loss: 1.39, generation_loss: 2.17\n",
      "epoch: 403, train acc: 90.00%, classification_loss: 0.12, discrimination_loss: 1.31, generation_loss: 2.01\n",
      "epoch: 404, train acc: 92.00%, classification_loss: 0.11, discrimination_loss: 1.41, generation_loss: 1.79\n",
      "epoch: 405, train acc: 92.00%, classification_loss: 0.11, discrimination_loss: 1.42, generation_loss: 2.17\n",
      "epoch: 406, train acc: 91.00%, classification_loss: 0.11, discrimination_loss: 1.41, generation_loss: 2.31\n",
      "epoch: 407, train acc: 88.00%, classification_loss: 0.11, discrimination_loss: 1.45, generation_loss: 2.08\n",
      "epoch: 408, train acc: 91.00%, classification_loss: 0.13, discrimination_loss: 1.31, generation_loss: 2.09\n",
      "epoch: 409, train acc: 90.00%, classification_loss: 0.13, discrimination_loss: 1.40, generation_loss: 2.08\n",
      "epoch: 410, train acc: 90.00%, classification_loss: 0.11, discrimination_loss: 1.43, generation_loss: 1.95\n",
      "epoch: 411, train acc: 90.00%, classification_loss: 0.12, discrimination_loss: 1.39, generation_loss: 2.18\n",
      "epoch: 412, train acc: 91.00%, classification_loss: 0.11, discrimination_loss: 1.34, generation_loss: 2.06\n",
      "epoch: 413, train acc: 91.00%, classification_loss: 0.11, discrimination_loss: 1.48, generation_loss: 1.85\n",
      "epoch: 414, train acc: 90.00%, classification_loss: 0.11, discrimination_loss: 1.37, generation_loss: 2.55\n",
      "epoch: 415, train acc: 91.00%, classification_loss: 0.14, discrimination_loss: 1.38, generation_loss: 1.98\n",
      "epoch: 416, train acc: 90.00%, classification_loss: 0.14, discrimination_loss: 1.37, generation_loss: 1.99\n",
      "epoch: 417, train acc: 91.00%, classification_loss: 0.13, discrimination_loss: 1.44, generation_loss: 2.15\n",
      "epoch: 418, train acc: 90.00%, classification_loss: 0.13, discrimination_loss: 1.38, generation_loss: 2.06\n",
      "epoch: 419, train acc: 92.00%, classification_loss: 0.14, discrimination_loss: 1.43, generation_loss: 1.96\n",
      "epoch: 420, train acc: 89.00%, classification_loss: 0.14, discrimination_loss: 1.32, generation_loss: 1.68\n",
      "epoch: 421, train acc: 90.00%, classification_loss: 0.12, discrimination_loss: 1.41, generation_loss: 1.94\n",
      "epoch: 422, train acc: 90.00%, classification_loss: 0.09, discrimination_loss: 1.45, generation_loss: 1.68\n",
      "epoch: 423, train acc: 91.00%, classification_loss: 0.14, discrimination_loss: 1.39, generation_loss: 2.13\n",
      "epoch: 424, train acc: 90.00%, classification_loss: 0.14, discrimination_loss: 1.35, generation_loss: 2.33\n",
      "epoch: 425, train acc: 90.00%, classification_loss: 0.09, discrimination_loss: 1.52, generation_loss: 2.22\n",
      "epoch: 426, train acc: 90.00%, classification_loss: 0.13, discrimination_loss: 1.33, generation_loss: 2.23\n",
      "epoch: 427, train acc: 92.00%, classification_loss: 0.12, discrimination_loss: 1.36, generation_loss: 2.19\n",
      "epoch: 428, train acc: 92.00%, classification_loss: 0.12, discrimination_loss: 1.38, generation_loss: 2.02\n",
      "epoch: 429, train acc: 94.00%, classification_loss: 0.12, discrimination_loss: 1.37, generation_loss: 2.20\n",
      "epoch: 430, train acc: 94.00%, classification_loss: 0.12, discrimination_loss: 1.35, generation_loss: 1.93\n",
      "epoch: 431, train acc: 91.00%, classification_loss: 0.12, discrimination_loss: 1.33, generation_loss: 1.81\n",
      "epoch: 432, train acc: 90.00%, classification_loss: 0.12, discrimination_loss: 1.38, generation_loss: 1.88\n",
      "epoch: 433, train acc: 89.00%, classification_loss: 0.14, discrimination_loss: 1.37, generation_loss: 2.11\n",
      "epoch: 434, train acc: 89.00%, classification_loss: 0.14, discrimination_loss: 1.34, generation_loss: 1.68\n",
      "epoch: 435, train acc: 86.00%, classification_loss: 0.12, discrimination_loss: 1.40, generation_loss: 2.55\n",
      "epoch: 436, train acc: 88.00%, classification_loss: 0.12, discrimination_loss: 1.41, generation_loss: 2.19\n",
      "epoch: 437, train acc: 89.00%, classification_loss: 0.11, discrimination_loss: 1.35, generation_loss: 2.03\n",
      "epoch: 438, train acc: 90.00%, classification_loss: 0.13, discrimination_loss: 1.49, generation_loss: 1.97\n",
      "epoch: 439, train acc: 90.00%, classification_loss: 0.13, discrimination_loss: 1.46, generation_loss: 2.04\n",
      "epoch: 440, train acc: 92.00%, classification_loss: 0.12, discrimination_loss: 1.32, generation_loss: 2.19\n",
      "epoch: 441, train acc: 89.00%, classification_loss: 0.12, discrimination_loss: 1.42, generation_loss: 1.92\n",
      "epoch: 442, train acc: 90.00%, classification_loss: 0.11, discrimination_loss: 1.41, generation_loss: 1.77\n",
      "epoch: 443, train acc: 90.00%, classification_loss: 0.10, discrimination_loss: 1.38, generation_loss: 1.91\n",
      "epoch: 444, train acc: 92.00%, classification_loss: 0.11, discrimination_loss: 1.41, generation_loss: 1.97\n",
      "epoch: 445, train acc: 94.00%, classification_loss: 0.11, discrimination_loss: 1.38, generation_loss: 1.88\n",
      "epoch: 446, train acc: 94.00%, classification_loss: 0.11, discrimination_loss: 1.34, generation_loss: 1.59\n",
      "epoch: 447, train acc: 91.00%, classification_loss: 0.11, discrimination_loss: 1.36, generation_loss: 1.87\n",
      "epoch: 448, train acc: 92.00%, classification_loss: 0.11, discrimination_loss: 1.31, generation_loss: 2.15\n",
      "epoch: 449, train acc: 91.00%, classification_loss: 0.11, discrimination_loss: 1.33, generation_loss: 2.06\n",
      "epoch: 450, train acc: 91.00%, classification_loss: 0.11, discrimination_loss: 1.43, generation_loss: 1.70\n",
      "epoch: 451, train acc: 90.00%, classification_loss: 0.11, discrimination_loss: 1.25, generation_loss: 1.98\n",
      "epoch: 452, train acc: 93.00%, classification_loss: 0.12, discrimination_loss: 1.40, generation_loss: 1.90\n",
      "epoch: 453, train acc: 93.00%, classification_loss: 0.12, discrimination_loss: 1.42, generation_loss: 1.89\n",
      "epoch: 454, train acc: 93.00%, classification_loss: 0.12, discrimination_loss: 1.42, generation_loss: 2.01\n",
      "epoch: 455, train acc: 94.00%, classification_loss: 0.12, discrimination_loss: 1.37, generation_loss: 1.78\n",
      "epoch: 456, train acc: 96.00%, classification_loss: 0.12, discrimination_loss: 1.38, generation_loss: 1.62\n",
      "epoch: 457, train acc: 95.00%, classification_loss: 0.12, discrimination_loss: 1.37, generation_loss: 1.92\n",
      "epoch: 458, train acc: 96.00%, classification_loss: 0.12, discrimination_loss: 1.30, generation_loss: 1.84\n",
      "epoch: 459, train acc: 96.00%, classification_loss: 0.12, discrimination_loss: 1.42, generation_loss: 1.83\n",
      "epoch: 460, train acc: 95.00%, classification_loss: 0.12, discrimination_loss: 1.33, generation_loss: 2.22\n",
      "epoch: 461, train acc: 94.00%, classification_loss: 0.12, discrimination_loss: 1.37, generation_loss: 1.61\n",
      "epoch: 462, train acc: 92.00%, classification_loss: 0.12, discrimination_loss: 1.45, generation_loss: 1.99\n",
      "epoch: 463, train acc: 90.00%, classification_loss: 0.12, discrimination_loss: 1.33, generation_loss: 2.02\n",
      "epoch: 464, train acc: 91.00%, classification_loss: 0.14, discrimination_loss: 1.41, generation_loss: 2.03\n",
      "epoch: 465, train acc: 90.00%, classification_loss: 0.14, discrimination_loss: 1.36, generation_loss: 1.76\n",
      "epoch: 466, train acc: 91.00%, classification_loss: 0.14, discrimination_loss: 1.24, generation_loss: 1.60\n",
      "epoch: 467, train acc: 89.00%, classification_loss: 0.14, discrimination_loss: 1.44, generation_loss: 1.86\n",
      "epoch: 468, train acc: 91.00%, classification_loss: 0.13, discrimination_loss: 1.48, generation_loss: 1.86\n",
      "epoch: 469, train acc: 90.00%, classification_loss: 0.13, discrimination_loss: 1.34, generation_loss: 1.90\n",
      "epoch: 470, train acc: 92.00%, classification_loss: 0.14, discrimination_loss: 1.53, generation_loss: 1.99\n",
      "epoch: 471, train acc: 85.00%, classification_loss: 0.14, discrimination_loss: 1.52, generation_loss: 1.97\n",
      "epoch: 472, train acc: 87.00%, classification_loss: 0.11, discrimination_loss: 1.58, generation_loss: 1.98\n",
      "epoch: 473, train acc: 90.00%, classification_loss: 0.14, discrimination_loss: 1.54, generation_loss: 1.70\n",
      "epoch: 474, train acc: 92.00%, classification_loss: 0.13, discrimination_loss: 1.45, generation_loss: 1.76\n",
      "epoch: 475, train acc: 92.00%, classification_loss: 0.13, discrimination_loss: 1.42, generation_loss: 1.36\n",
      "epoch: 476, train acc: 91.00%, classification_loss: 0.13, discrimination_loss: 1.38, generation_loss: 1.87\n",
      "epoch: 477, train acc: 92.00%, classification_loss: 0.13, discrimination_loss: 1.51, generation_loss: 1.89\n",
      "epoch: 478, train acc: 93.00%, classification_loss: 0.13, discrimination_loss: 1.35, generation_loss: 1.54\n",
      "epoch: 479, train acc: 93.00%, classification_loss: 0.13, discrimination_loss: 1.51, generation_loss: 1.86\n",
      "epoch: 480, train acc: 93.00%, classification_loss: 0.13, discrimination_loss: 1.17, generation_loss: 1.89\n",
      "epoch: 481, train acc: 91.00%, classification_loss: 0.13, discrimination_loss: 1.48, generation_loss: 1.95\n",
      "epoch: 482, train acc: 91.00%, classification_loss: 0.13, discrimination_loss: 1.47, generation_loss: 1.49\n",
      "epoch: 483, train acc: 91.00%, classification_loss: 0.13, discrimination_loss: 1.43, generation_loss: 1.63\n",
      "epoch: 484, train acc: 89.00%, classification_loss: 0.13, discrimination_loss: 1.39, generation_loss: 1.70\n",
      "epoch: 485, train acc: 91.00%, classification_loss: 0.13, discrimination_loss: 1.38, generation_loss: 1.57\n",
      "epoch: 486, train acc: 91.00%, classification_loss: 0.13, discrimination_loss: 1.43, generation_loss: 1.79\n",
      "epoch: 487, train acc: 89.00%, classification_loss: 0.13, discrimination_loss: 1.48, generation_loss: 2.23\n",
      "epoch: 488, train acc: 92.00%, classification_loss: 0.14, discrimination_loss: 1.30, generation_loss: 1.88\n",
      "epoch: 489, train acc: 91.00%, classification_loss: 0.14, discrimination_loss: 1.33, generation_loss: 1.72\n",
      "epoch: 490, train acc: 90.00%, classification_loss: 0.14, discrimination_loss: 1.38, generation_loss: 1.95\n",
      "epoch: 491, train acc: 94.00%, classification_loss: 0.12, discrimination_loss: 1.34, generation_loss: 1.68\n",
      "epoch: 492, train acc: 92.00%, classification_loss: 0.12, discrimination_loss: 1.36, generation_loss: 1.65\n",
      "epoch: 493, train acc: 92.00%, classification_loss: 0.12, discrimination_loss: 1.40, generation_loss: 1.74\n",
      "epoch: 494, train acc: 92.00%, classification_loss: 0.12, discrimination_loss: 1.41, generation_loss: 1.76\n",
      "epoch: 495, train acc: 92.00%, classification_loss: 0.12, discrimination_loss: 1.34, generation_loss: 2.06\n",
      "epoch: 496, train acc: 91.00%, classification_loss: 0.12, discrimination_loss: 1.49, generation_loss: 1.64\n",
      "epoch: 497, train acc: 90.00%, classification_loss: 0.12, discrimination_loss: 1.32, generation_loss: 1.83\n",
      "epoch: 498, train acc: 91.00%, classification_loss: 0.16, discrimination_loss: 1.31, generation_loss: 1.83\n",
      "epoch: 499, train acc: 90.00%, classification_loss: 0.16, discrimination_loss: 1.35, generation_loss: 1.77\n"
     ]
    }
   ],
   "source": [
    "model.fit(data_X, data_Y, epochs=500, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 683,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6123999953269958"
      ]
     },
     "execution_count": 683,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.validation(data_X, data_Y)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "GAN",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
