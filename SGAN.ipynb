{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch import Tensor\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision.datasets import MNIST\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from CNNmodel import ConvModel\n",
    "from utils import * "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RANDOM_SEED  :  110404\n",
      "DATA_DIR     :  ./data\n",
      "USED_DATA    :   MNIST\n",
      "NUM_LABELLED :    1000\n",
      "DEVICE       :  cuda:0\n",
      "EPOCHS       :      50\n",
      "BATCH_SIZE   :      64\n",
      "LEARNING_RATE:    0.01\n"
     ]
    }
   ],
   "source": [
    "print_config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_seed = config.RANDOM_SEED\n",
    "torch.manual_seed(random_seed)\n",
    "random.seed(random_seed)\n",
    "np.random.seed(random_seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = \"./data\"\n",
    "n_classes = 10\n",
    "device = config.DEVICE\n",
    "num_labelled = config.NUM_LABELLED\n",
    "epochs = config.EPOCHS\n",
    "batch_size = config.BATCH_SIZE\n",
    "learning_rate = config.LEARNING_RATE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train, X_test, y_test, classes = load_data(0, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_sup, y_sup, X_unsup, y_unsup = supervised_samples(X_train, y_train, config.NUM_LABELLED, n_classes, get_unsup=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1000, 1, 28, 28])\n"
     ]
    }
   ],
   "source": [
    "print(X_sup.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([60000, 1, 28, 28])\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "X_train = X_train.to(device, dtype = torch.float32) \n",
    "X_test = X_test.to(device, dtype = torch.float32)\n",
    "print(X_train.get_device())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = y_train.to(device)\n",
    "y_test = y_test.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([5, 0, 4,  ..., 5, 6, 8], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "print(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "\tdef __init__(self, inp_size, out_size) -> None:\n",
    "\t\tsuper().__init__()\n",
    "\n",
    "\n",
    "\t\tself.NN = nn.Sequential(\n",
    "\t\t\tnn.Linear(inp_size, 256*7*7),\n",
    "\t\t\tnn.LeakyReLU(negative_slope=0.2),\n",
    "\t\t)\n",
    "\n",
    "\t\tself.CONV = nn.Sequential(\n",
    "\t\t\tnn.ConvTranspose2d(256, 128, (3, 3), (2, 2)),\n",
    "\t\t\tnn.BatchNorm2d(128),\n",
    "\t\t\tnn.LeakyReLU(0.2),\n",
    "\t\t\tnn.ConvTranspose2d(128, 64, (3, 3), (1, 1)),\n",
    "\t\t\tnn.BatchNorm2d(64),\n",
    "\t\t\tnn.LeakyReLU(0.2),\n",
    "\t\t\t\n",
    "\t\t)\n",
    "\n",
    "\t\tself.out = nn.Sequential(\n",
    "\t\t\tnn.ConvTranspose2d(64, out_size[0], (3, 3), (2, 2)),\n",
    "\t\t\tnn.AdaptiveAvgPool2d((out_size[1], out_size[2])),\n",
    "\t\t\tnn.Sigmoid()\n",
    "\t\t)\n",
    "\n",
    "\t\tself.optimizer = optim.Adam(self.parameters(), lr = 0.0002, betas=[0.5, 0.999])\n",
    "\t\tself.criterion = nn.BCELoss()\n",
    "\t\n",
    "\tdef forward(self, X: Tensor):\n",
    "\t\tX = self.NN(X)\n",
    "\t\tX = X.view(-1, 256, 7, 7)\n",
    "\t\tX = self.CONV(X)\n",
    "\n",
    "\t\tX = self.out(X)\n",
    "\n",
    "\t\treturn X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Feature_Extractor(nn.Module):\n",
    "\tdef __init__(self, inp_channel) -> None:\n",
    "\t\tsuper().__init__()\n",
    "\n",
    "\n",
    "\t\tself.CNN = ConvModel(inp_channel)\n",
    "\n",
    "\t\tself.dropout = nn.Sequential(\n",
    "\t\t\tnn.Dropout(0.4)\n",
    "\t\t)\n",
    "\n",
    "\t\n",
    "\tdef forward(self, X: Tensor):\n",
    "\t\tX = self.CNN(X)\n",
    "\t\tX = self.dropout(X)\n",
    "\t\treturn X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Classify(nn.Module):\n",
    "\tdef __init__(self, feature_extractor: nn.Module, num_classes) -> None:\n",
    "\t\tsuper().__init__()\n",
    "\n",
    "\t\tself.CNN = feature_extractor\n",
    "\n",
    "\t\tself.out = nn.Sequential(\n",
    "\t\t\tnn.Linear(512, num_classes)\n",
    "\t\t)\n",
    "\n",
    "\t\tself.optimizer = optim.Adam(self.parameters(), lr = 0.0002, betas= [0.5, 0.999])\n",
    "\n",
    "\t\tself.criterion = nn.CrossEntropyLoss()\n",
    "\t\n",
    "\t\n",
    "\tdef forward(self, X: Tensor):\n",
    "\t\tX = self.CNN(X)\n",
    "\t\tX = self.out(X)\n",
    "\n",
    "\t\treturn X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discriminator(nn.Module):\n",
    "\tdef __init__(self, feature_extractor) -> None:\n",
    "\t\tsuper().__init__()\n",
    "\t\tself.CNN = feature_extractor\n",
    "\n",
    "\t\tself.out = nn.Sequential(\n",
    "\t\t\tnn.Linear(512, 1),\n",
    "\t\t\tnn.Sigmoid()\n",
    "\t\t)\n",
    "\t\n",
    "\t\tself.optimizer = optim.Adam(self.parameters(), lr=0.0002, betas=[0.5, 0.999])\n",
    "\t\tself.criterion = nn.BCELoss()\n",
    "\n",
    "\tdef forward(self, X: Tensor):\n",
    "\t\tX = self.CNN(X)\n",
    "\n",
    "\t\tX = self.out(X)\n",
    "\n",
    "\t\treturn X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SGAN:\n",
    "\tdef __init__(self, image_size, num_classes, feature_extractor: nn.Module, latent_size = 100, lr=0.0002):\n",
    "\n",
    "\t\tself.latent_size = latent_size\n",
    "\n",
    "\t\tCNN = feature_extractor.to(device)\n",
    "\n",
    "\t\tself.generator = Generator(latent_size, image_size).to(device)\n",
    "\n",
    "\t\tself.classify = Classify(CNN, num_classes).to(device)\n",
    "\t\tself.discriminator = Discriminator(CNN).to(device)\n",
    "\n",
    "\t\tself.history = {}\n",
    "\t\n",
    "\tdef __call__(self, X: torch.Tensor):\n",
    "\t\treturn torch.argmax(self.classify(X))\n",
    "\t\n",
    "\tdef save(self, PATH = \"./\"):\n",
    "\t\ttorch.save(self.classify.state_dict(), PATH + \"/classify.pt\")\n",
    "\t\ttorch.save(self.discriminator.state_dict(), PATH + \"/discriminator.pt\")\n",
    "\t\ttorch.save(self.generator.state_dict(), PATH + \"/generator.pt\")\n",
    "\t\n",
    "\tdef validation(self, X: Tensor, y: Tensor):\n",
    "\t\tself.classify.eval()\n",
    "\n",
    "\t\tnum_data = y.shape[0]\n",
    "\n",
    "\t\trun_size = 10000\n",
    "\n",
    "\t\tcurrent = 0\n",
    "\t\tcorrect = 0\n",
    "\t\t\n",
    "\t\twhile current < num_data:\n",
    "\t\t\tcorrect += torch.count_nonzero(torch.argmax(self.classify(X[current: current + run_size]), 1) == torch.argmax(y[current: current + run_size], 1))\n",
    "\t\t\tcurrent += run_size\n",
    "\n",
    "\t\treturn (correct.float().item() / num_data)\n",
    "\n",
    "\n",
    "\tdef training_step(self, model: nn.Module, optimizer: optim.Optimizer, criterion: nn.modules.loss._Loss, X: Tensor, y: Tensor):\n",
    "\t\tout: Tensor = model(X)\n",
    "\t\tloss: Tensor = criterion(out, y)\n",
    "\t\t\n",
    "\n",
    "\t\toptimizer.zero_grad()\n",
    "\t\tloss.backward()\n",
    "\t\toptimizer.step()\n",
    "\n",
    "\t\treturn loss\n",
    "\t\n",
    "\n",
    "\tdef fit(self, X: Tensor, y: Tensor, X_sup: Tensor, y_sup: Tensor, sup_samples, epochs = 100, batch_size = 64, val: bool = True, gen_epochs = 25):\n",
    "\n",
    "\t\tX_sup, y_sup, X_val, y_val = supervised_sampling(X_sup, y_sup, sup_samples)\n",
    "\n",
    "\t\tdatasets = CustomDataSet(X, y)\n",
    "\n",
    "\t\tsup_datasets = CustomDataSet(X_sup, y_sup)\n",
    "\n",
    "\t\tdataloader = DataLoader(datasets, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "\t\tsup_dataloader = DataLoader(sup_datasets, batch_size=batch_size//2, shuffle=True)\n",
    "\n",
    "\t\tfor epoch in range(epochs):\n",
    "\t\t\tself.classify.train()\n",
    "\t\t\tself.discriminator.train()\n",
    "\t\t\tself.generator.train()\n",
    "\n",
    "\t\t\t# print(f\"epoch: {epoch}\\nclassify: \")\n",
    "\t\t\t\n",
    "\t\t\t# for classify\n",
    "\t\t\tfor inputs, labels in tqdm(sup_dataloader):\n",
    "\t\t\t\tsup_loss = self.training_step(self.classify, self.classify.optimizer, self.classify.criterion, inputs.to(device), labels.to(device))\n",
    "\n",
    "\t\t\t# print(f'GAN:')\n",
    "\t\t\t# for discriminator and generator\n",
    "\t\t\tif (epoch < gen_epochs): \n",
    "\t\t\t\tfor inputs, _ in tqdm(dataloader):\n",
    "\t\t\t\t\treal_loss = self.training_step(self.discriminator, self.discriminator.optimizer, self.discriminator.criterion, inputs, torch.ones((inputs.shape[0], 1)).to(device))\n",
    "\t\t\t\t\t\n",
    "\t\t\t\t\tz = torch.randn((inputs.shape[0], self.latent_size)).to(device)\n",
    "\t\t\t\t\tgen_out = self.generator(z)\n",
    "\t\t\t\t\tfake_loss = self.training_step(self.discriminator, self.discriminator.optimizer, self.discriminator.criterion, gen_out, torch.zeros(inputs.shape[0], 1).to(device))\n",
    "\t\t\t\t\tgen_out = self.generator(z)\n",
    "\t\t\t\t\tgen_loss = self.training_step(self.discriminator, self.generator.optimizer, self.discriminator.criterion, gen_out, torch.ones((inputs.shape[0], 1)).to(device))\n",
    "\n",
    "\t\t\t\tif val: \n",
    "\t\t\t\t\ttrain_acc = self.validation(X_sup, y_sup)\n",
    "\n",
    "\t\t\t\t\tval_acc = self.validation(X_val, y_val)\n",
    "\t\t\t\t\t\n",
    "\t\t\t\t\tprint(f\"train acc: {train_acc*100:.2f}%, val acc: {val_acc*100:.2f}%, classification_loss: {sup_loss:.2f}, discrimination_loss: {(real_loss+fake_loss)/2:.2f}, generation_loss: {gen_loss:.2f}\")\n",
    "\t\t\t\n",
    "\t\t\telse: \n",
    "\t\t\t\tif val:\n",
    "\t\t\t\t\tprint(f\"classification_loss: {sup_loss:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy \n",
    "from torch import IntTensor\n",
    "class SelfTraining: \n",
    "    def __init__(self, model: SGAN, X: Tensor, y: Tensor, num_rounds, sup_samples: int): \n",
    "        '''\n",
    "            Input of self-training model:\n",
    "            model: SGAN\n",
    "            labeled_dataset: labelled dataset \n",
    "            unlabeled_dataset: unlabelled dataset \n",
    "            num_rounds: number of self_training rounds\n",
    "        '''\n",
    "        self.model = model \n",
    "        self.X = X \n",
    "        self.y = y\n",
    "        X_sup, y_sup, X_unsup, y_unsup = supervised_samples(X, y, config.NUM_LABELLED, n_classes, get_unsup=True)\n",
    "        X_sup.to(device) \n",
    "        y_sup.to(device) \n",
    "        X_unsup.to(device) \n",
    "        y_unsup.to(device) \n",
    "\n",
    "        print(X_sup.get_device())\n",
    "        print(X_unsup.get_device())\n",
    "        self.labeled_dataset = CustomDataSet(X_sup, y_sup)\n",
    "        self.unlabeled_dataset = CustomDataSet(X_unsup, y_unsup)\n",
    "        self.num_rounds = num_rounds\n",
    "        self.sup_samples = sup_samples\n",
    "        \n",
    "    def CalDisagreement(self, h1: Classify, h2: Classify, dataset: CustomDataSet): \n",
    "        '''\n",
    "            Calculate disagreement between teacher model and student model\n",
    "            h1: Teacher model \n",
    "            h2: Student model\n",
    "        '''\n",
    "        disagreement = 0\n",
    "        for x, _ in dataset: \n",
    "            disagreement += (torch.argmax(h1(x.unsqueeze(0))) == torch.argmax(h2(x.unsqueeze(0))))\n",
    "        \n",
    "        return disagreement/len(dataset)\n",
    "    def training_step(self, model: nn.Module, optimizer: optim.Optimizer, criterion: nn.modules.loss._Loss, X: Tensor, y: Tensor):\n",
    "        out: Tensor = model(X).to(dtype = torch.float)\n",
    "        y = y.to(dtype=torch.float)\n",
    "        loss: Tensor = criterion(out, y)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "\n",
    "    def random_sampling(self, sample_fraction: float, dataset: CustomDataSet, n: int): \n",
    "        dataset_set: list[CustomDataSet] = []\n",
    "        for _ in range(n): \n",
    "            \n",
    "            idx = random.sample(range(0, len(dataset)), int(len(dataset)*sample_fraction))\n",
    "            data_X = dataset.x[idx]\n",
    "            data_y = dataset.y[idx]\n",
    "\n",
    "            dataset_set.append(CustomDataSet(data_X, data_y))\n",
    "        return dataset_set\n",
    "\n",
    "    def selfTraining(self, batch_size: int, sample_fraction: float, n: int): \n",
    "        labeled_dataset = copy.copy(self.labeled_dataset)\n",
    "        unlabeled_dataset = copy.copy(self.unlabeled_dataset)\n",
    "        teacher_model = copy.copy(self.model)\n",
    "        for _ in range(self.num_rounds): \n",
    "            student_model = copy.copy(teacher_model)\n",
    "           \n",
    "            student_model.fit(self.X, \n",
    "                              self.y, \n",
    "                              labeled_dataset.x, \n",
    "                              labeled_dataset.y,\n",
    "                              sup_samples=self.sup_samples, epochs = epochs, batch_size = batch_size)\n",
    "            d=dict()\n",
    "            labels = []\n",
    "            for x in (unlabeled_dataset.x): \n",
    "                # print(x.get_device())\n",
    "                c_labels = student_model.classify(x.unsqueeze(0))\n",
    "                # print(c_labels[0])\n",
    "                labels.append(c_labels)\n",
    "                a = torch.sum(Tensor([i*c_labels[0][i] for i in range(len(c_labels[0]))]))\n",
    "                # print(a.shape)\n",
    "                if not isinstance(a, Tensor): \n",
    "                    print(type(a))\n",
    "                d[x] = a\n",
    "            \n",
    "            threshold = np.median(np.array(list(d.values())))\n",
    "            threshold_X = Tensor().to(device)\n",
    "            threshold_y = IntTensor().to(device)\n",
    "            threshold_idx = IntTensor().to(device)\n",
    "        \n",
    "            for i, x in enumerate(d):\n",
    "                if d[x] > threshold:\n",
    "                    threshold_X = torch.cat((threshold_X, x.unsqueeze(0)))\n",
    "                    threshold_y = torch.cat((threshold_y, Tensor(torch.argmax(labels[i]).to(device).unsqueeze(0))))\n",
    "            threshold_ds = CustomDataSet(threshold_X, threshold_y)\n",
    "            # randomly sample sample_fraction of threshold_ds\n",
    "            dataset_set = self.random_sampling(sample_fraction=sample_fraction, dataset=threshold_ds, n=n)\n",
    "            max = 0\n",
    "            for I in range(len(dataset_set)): \n",
    "                model = Classify(ConvModel(1), n_classes).to(device)\n",
    "                model.train()\n",
    "                # calculate U\\U[i]\n",
    "                unlabel = unlabeled_dataset.x\n",
    "                unlabel_i = dataset_set[I].x\n",
    "\n",
    "                counterpart = Tensor().type_as(unlabel)\n",
    "                \n",
    "                # '''debugging'''\n",
    "                # testing = True\n",
    "                # print(threshold_X.shape) \n",
    "                # for i in range(60): \n",
    "                #     for j in range(i, 61): \n",
    "                #         if torch.equal(threshold_X[i] ,threshold_X[j]): \n",
    "                #             testing = False\n",
    "                \n",
    "                # print(testing)\n",
    "                # break\n",
    "\n",
    "                for i in range(unlabel.shape[0]): \n",
    "                    check = True\n",
    "                    for j in range(unlabel_i.shape[0]): \n",
    "                       if torch.equal(unlabel[i], unlabel_i[j]):\n",
    "                           check = False\n",
    "                           break\n",
    "                    if check:\n",
    "                       counterpart = torch.cat((counterpart, unlabel[i].unsqueeze(0)))\n",
    "\n",
    "\n",
    "                # generate label of data in U\\U[i] by teacher_model classifier \n",
    "                y_counterpart = Tensor().type_as(unlabel)\n",
    "                for x in counterpart: \n",
    "                    y_counterpart = torch.cat((y_counterpart, teacher_model(x.unsqueeze(0)).unsqueeze(0)))\n",
    "\n",
    "                X_data = torch.cat((labeled_dataset.x, unlabel_i, counterpart))\n",
    "                y_data = one_hot(torch.cat((labeled_dataset.y, dataset_set[I].y ,y_counterpart)).to(dtype=torch.int))\n",
    "               \n",
    "                print(y_data.shape)\n",
    "                print(model(X_data).shape)\n",
    "\n",
    "\n",
    "                self.training_step(model, model.optimizer, model.criterion, X_data.to(device), y_data.to(device))\n",
    "                if self.CalDisagreement(student_model.classify, model, unlabeled_dataset) > max: \n",
    "                    max = self.CalDisagreement(student_model, model, unlabeled_dataset)\n",
    "                    best = dataset_set[I]\n",
    "            \n",
    "            labeled_dataset.x = torch.cat((labeled_dataset.x, best.x))\n",
    "            labeled_dataset.y = torch.cat((labeled_dataset.y, best.y))\n",
    "            # remove sample from best dataset from unlabled dataset \n",
    "            for i in range(unlabeled_dataset.x.shape[0]): \n",
    "                check = True\n",
    "                for j in range(best.x.shape[0]): \n",
    "                    if torch.equal(unlabeled_dataset.x[i], best.x[j]):\n",
    "                        check = False\n",
    "                        break\n",
    "                    if not check:\n",
    "                        unlabeled_dataset.x = torch.cat((unlabeled_dataset.x[:i], unlabeled_dataset.x[:i+1]))\n",
    "                        unlabeled_dataset.y = torch.cat((unlabeled_dataset.y[:i], unlabeled_dataset.y[:i+1]))\n",
    "            # reassign teacher model \n",
    "            teacher_model = student_model\n",
    "        \n",
    "        # return best model \n",
    "        self.model = teacher_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SGAN([1, 28, 28], n_classes, ConvModel(1), lr= learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "self_training = SelfTraining(model, X_train, y_train, num_rounds = 10, sup_samples = num_labelled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 25/25 [00:01<00:00, 23.95it/s]\n",
      "100%|██████████| 938/938 [01:13<00:00, 12.74it/s]\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "Dimension out of range (expected to be in range of [-1, 0], but got 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[30], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mself_training\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mselfTraining\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m64\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0.4\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[27], line 67\u001b[0m, in \u001b[0;36mSelfTraining.selfTraining\u001b[0;34m(self, batch_size, sample_fraction, n)\u001b[0m\n\u001b[1;32m     64\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_rounds): \n\u001b[1;32m     65\u001b[0m     student_model \u001b[38;5;241m=\u001b[39m copy\u001b[38;5;241m.\u001b[39mcopy(teacher_model)\n\u001b[0;32m---> 67\u001b[0m     \u001b[43mstudent_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     68\u001b[0m \u001b[43m                      \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     69\u001b[0m \u001b[43m                      \u001b[49m\u001b[43mlabeled_dataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     70\u001b[0m \u001b[43m                      \u001b[49m\u001b[43mlabeled_dataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     71\u001b[0m \u001b[43m                      \u001b[49m\u001b[43msup_samples\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msup_samples\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     72\u001b[0m     d\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mdict\u001b[39m()\n\u001b[1;32m     73\u001b[0m     labels \u001b[38;5;241m=\u001b[39m []\n",
      "Cell \u001b[0;32mIn[19], line 88\u001b[0m, in \u001b[0;36mSGAN.fit\u001b[0;34m(self, X, y, X_sup, y_sup, sup_samples, epochs, batch_size, val, gen_epochs)\u001b[0m\n\u001b[1;32m     85\u001b[0m \tgen_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining_step(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdiscriminator, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgenerator\u001b[38;5;241m.\u001b[39moptimizer, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdiscriminator\u001b[38;5;241m.\u001b[39mcriterion, gen_out, torch\u001b[38;5;241m.\u001b[39mones((inputs\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m], \u001b[38;5;241m1\u001b[39m))\u001b[38;5;241m.\u001b[39mto(device))\n\u001b[1;32m     87\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m val: \n\u001b[0;32m---> 88\u001b[0m \ttrain_acc \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalidation\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_sup\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_sup\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     90\u001b[0m \tval_acc \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvalidation(X_val, y_val)\n\u001b[1;32m     92\u001b[0m \t\u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain acc: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrain_acc\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m100\u001b[39m\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m%, val acc: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mval_acc\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m100\u001b[39m\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m%, classification_loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msup_loss\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, discrimination_loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m(real_loss\u001b[38;5;241m+\u001b[39mfake_loss)\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m2\u001b[39m\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, generation_loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mgen_loss\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[19], line 34\u001b[0m, in \u001b[0;36mSGAN.validation\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m     31\u001b[0m correct \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m     33\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m current \u001b[38;5;241m<\u001b[39m num_data:\n\u001b[0;32m---> 34\u001b[0m \tcorrect \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcount_nonzero(torch\u001b[38;5;241m.\u001b[39margmax(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclassify(X[current: current \u001b[38;5;241m+\u001b[39m run_size]), \u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m==\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43margmax\u001b[49m\u001b[43m(\u001b[49m\u001b[43my\u001b[49m\u001b[43m[\u001b[49m\u001b[43mcurrent\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mcurrent\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mrun_size\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m)\n\u001b[1;32m     35\u001b[0m \tcurrent \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m run_size\n\u001b[1;32m     37\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m (correct\u001b[38;5;241m.\u001b[39mfloat()\u001b[38;5;241m.\u001b[39mitem() \u001b[38;5;241m/\u001b[39m num_data)\n",
      "\u001b[0;31mIndexError\u001b[0m: Dimension out of range (expected to be in range of [-1, 0], but got 1)"
     ]
    }
   ],
   "source": [
    "self_training.selfTraining(batch_size, 0.4, 10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "GAN",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
