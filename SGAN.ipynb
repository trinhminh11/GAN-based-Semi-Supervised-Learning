{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch import Tensor\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision.datasets import MNIST\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from CNNmodel import ConvModel\n",
    "from utils import * "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_seed = 0\n",
    "torch.manual_seed(random_seed)\n",
    "random.seed(random_seed)\n",
    "np.random.seed(random_seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=0)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DATA_DIR = \"./data\"\n",
    "n_classes = 10\n",
    "num_labelled = 125\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train, X_test, y_test, classes = load_data(0, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train = supervised_samples(X_train, y_train, config.NUM_LABELLED, n_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "X_train = X_train.to(device, dtype = torch.float32) \n",
    "X_test = X_test.to(device, dtype = torch.float32)\n",
    "print(X_train.get_device())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = y_train.to(device)\n",
    "y_test = y_test.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0, 0, 0,  ..., 9, 9, 9], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "print(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "\tdef __init__(self, inp_size, out_size) -> None:\n",
    "\t\tsuper().__init__()\n",
    "\n",
    "\n",
    "\t\tself.NN = nn.Sequential(\n",
    "\t\t\tnn.Linear(inp_size, 256*7*7),\n",
    "\t\t\tnn.LeakyReLU(negative_slope=0.2),\n",
    "\t\t)\n",
    "\n",
    "\t\tself.CONV = nn.Sequential(\n",
    "\t\t\tnn.ConvTranspose2d(256, 128, (3, 3), (2, 2)),\n",
    "\t\t\tnn.BatchNorm2d(128),\n",
    "\t\t\tnn.LeakyReLU(0.2),\n",
    "\t\t\tnn.ConvTranspose2d(128, 64, (3, 3), (1, 1)),\n",
    "\t\t\tnn.BatchNorm2d(64),\n",
    "\t\t\tnn.LeakyReLU(0.2),\n",
    "\t\t\t\n",
    "\t\t)\n",
    "\n",
    "\t\tself.out = nn.Sequential(\n",
    "\t\t\tnn.ConvTranspose2d(64, out_size[0], (3, 3), (2, 2)),\n",
    "\t\t\tnn.AdaptiveAvgPool2d((out_size[1], out_size[2])),\n",
    "\t\t\tnn.Sigmoid()\n",
    "\t\t)\n",
    "\n",
    "\t\tself.optimizer = optim.Adam(self.parameters(), lr = 0.0002, betas=[0.5, 0.999])\n",
    "\t\tself.criterion = nn.BCELoss()\n",
    "\t\n",
    "\tdef forward(self, X: Tensor):\n",
    "\t\tX = self.NN(X)\n",
    "\t\tX = X.view(-1, 256, 7, 7)\n",
    "\t\tX = self.CONV(X)\n",
    "\n",
    "\t\tX = self.out(X)\n",
    "\n",
    "\t\treturn X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Feature_Extractor(nn.Module):\n",
    "\tdef __init__(self, inp_channel) -> None:\n",
    "\t\tsuper().__init__()\n",
    "\n",
    "\n",
    "\t\tself.CNN = ConvModel(inp_channel)\n",
    "\n",
    "\t\tself.dropout = nn.Sequential(\n",
    "\t\t\tnn.Dropout(0.4)\n",
    "\t\t)\n",
    "\n",
    "\t\n",
    "\tdef forward(self, X: Tensor):\n",
    "\t\tX = self.CNN(X)\n",
    "\t\tX = self.dropout(X)\n",
    "\t\treturn X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Classify(nn.Module):\n",
    "\tdef __init__(self, feature_extractor: nn.Module, num_classes) -> None:\n",
    "\t\tsuper().__init__()\n",
    "\n",
    "\t\tself.CNN = feature_extractor\n",
    "\n",
    "\t\tself.out = nn.Sequential(\n",
    "\t\t\tnn.Linear(512, num_classes)\n",
    "\t\t)\n",
    "\n",
    "\t\tself.optimizer = optim.Adam(self.parameters(), lr = 0.0002, betas= [0.5, 0.999])\n",
    "\n",
    "\t\tself.criterion = nn.CrossEntropyLoss()\n",
    "\t\n",
    "\t\n",
    "\tdef forward(self, X: Tensor):\n",
    "\t\tX = self.CNN(X)\n",
    "\t\tX = self.out(X)\n",
    "\n",
    "\t\treturn X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discriminator(nn.Module):\n",
    "\tdef __init__(self, feature_extractor) -> None:\n",
    "\t\tsuper().__init__()\n",
    "\t\tself.CNN = feature_extractor\n",
    "\n",
    "\t\tself.out = nn.Sequential(\n",
    "\t\t\tnn.Linear(512, 1),\n",
    "\t\t\tnn.Sigmoid()\n",
    "\t\t)\n",
    "\t\n",
    "\t\tself.optimizer = optim.Adam(self.parameters(), lr=0.0002, betas=[0.5, 0.999])\n",
    "\t\tself.criterion = nn.BCELoss()\n",
    "\n",
    "\tdef forward(self, X: Tensor):\n",
    "\t\tX = self.CNN(X)\n",
    "\n",
    "\t\tX = self.out(X)\n",
    "\n",
    "\t\treturn X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SGAN:\n",
    "\tdef __init__(self, image_size, num_classes, feature_extractor: nn.Module, latent_size = 100, lr=0.0002):\n",
    "\n",
    "\t\tself.latent_size = latent_size\n",
    "\n",
    "\t\tCNN = feature_extractor.to(device)\n",
    "\n",
    "\t\tself.generator = Generator(latent_size, image_size).to(device)\n",
    "\n",
    "\t\tself.classify = Classify(CNN, num_classes).to(device)\n",
    "\t\tself.discriminator = Discriminator(CNN).to(device)\n",
    "\n",
    "\t\tself.history = {}\n",
    "\t\n",
    "\tdef __call__(self, X: torch.Tensor):\n",
    "\t\treturn torch.argmax(self.classify(X))\n",
    "\t\n",
    "\tdef save(self, PATH = \"./\"):\n",
    "\t\ttorch.save(self.classify.state_dict(), PATH + \"/classify.pt\")\n",
    "\t\ttorch.save(self.discriminator.state_dict(), PATH + \"/discriminator.pt\")\n",
    "\t\ttorch.save(self.generator.state_dict(), PATH + \"/generator.pt\")\n",
    "\t\n",
    "\tdef validation(self, X: Tensor, y: Tensor):\n",
    "\t\tself.classify.eval()\n",
    "\n",
    "\t\tnum_data = y.shape[0]\n",
    "\n",
    "\t\trun_size = 10000\n",
    "\n",
    "\t\tcurrent = 0\n",
    "\t\tcorrect = 0\n",
    "\t\t\n",
    "\t\twhile current < num_data:\n",
    "\t\t\tcorrect += torch.count_nonzero(torch.argmax(self.classify(X[current: current + run_size]), 1) == torch.argmax(y[current: current + run_size], 1))\n",
    "\t\t\tcurrent += run_size\n",
    "\n",
    "\t\treturn (correct.float().item() / num_data)\n",
    "\n",
    "\n",
    "\tdef training_step(self, model: nn.Module, optimizer: optim.Optimizer, criterion: nn.modules.loss._Loss, X: Tensor, y: Tensor):\n",
    "\t\tout: Tensor = model(X)\n",
    "\t\tloss: Tensor = criterion(out, y)\n",
    "\t\t\n",
    "\n",
    "\t\toptimizer.zero_grad()\n",
    "\t\tloss.backward()\n",
    "\t\toptimizer.step()\n",
    "\n",
    "\t\treturn loss\n",
    "\t\n",
    "\n",
    "\tdef fit(self, X: Tensor, y: Tensor,X_val: Tensor, y_val: Tensor, sup_samples, epochs = 100, batch_size = 64, val: bool = False):\n",
    "\n",
    "\t\tX_sup, y_sup = supervised_samples(X, y, sup_samples, -1)\n",
    "\n",
    "\t\tdatasets = CustomDataSet(X, y)\n",
    "\n",
    "\t\tsup_datasets = CustomDataSet(X_sup, y_sup)\n",
    "\n",
    "\t\tdataloader = DataLoader(datasets, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "\t\tsup_dataloader = DataLoader(sup_datasets, batch_size=batch_size//2, shuffle=True)\n",
    "\n",
    "\t\tfor epoch in range(epochs):\n",
    "\t\t\tself.classify.train()\n",
    "\t\t\tself.discriminator.train()\n",
    "\t\t\tself.generator.train()\n",
    "\n",
    "\t\t\t# print(f\"epoch: {epoch}\\nclassify: \")\n",
    "\t\t\t\n",
    "\t\t\t# for classify\n",
    "\t\t\tfor inputs, labels in tqdm(sup_dataloader):\n",
    "\t\t\t\tsup_loss = self.training_step(self.classify, self.classify.optimizer, self.classify.criterion, inputs.to(device), labels.to(device))\n",
    "\n",
    "\t\t\t# print(f'GAN:')\n",
    "\t\t\t# for discriminator and generator\n",
    "\t\t\tfor inputs, _ in tqdm(dataloader):\n",
    "\t\t\t\treal_loss = self.training_step(self.discriminator, self.discriminator.optimizer, self.discriminator.criterion, inputs, torch.ones((inputs.shape[0], 1)).to(device))\n",
    "\t\t\t\t\n",
    "\t\t\t\tz = torch.randn((inputs.shape[0], self.latent_size)).to(device)\n",
    "\t\t\t\tgen_out = self.generator(z)\n",
    "\t\t\t\tfake_loss = self.training_step(self.discriminator, self.discriminator.optimizer, self.discriminator.criterion, gen_out, torch.zeros(inputs.shape[0], 1).to(device))\n",
    "\t\t\t\tgen_out = self.generator(z)\n",
    "\t\t\t\tgen_loss = self.training_step(self.discriminator, self.generator.optimizer, self.discriminator.criterion, gen_out, torch.ones((inputs.shape[0], 1)).to(device))\n",
    "\n",
    "\t\t\ttrain_acc = self.validation(X_sup, y_sup)\n",
    "\n",
    "\t\t\tval_acc = self.validation(X_val, y_val)\n",
    "\t\t\t\n",
    "\t\t\tprint(f\"train acc: {train_acc*100:.2f}%, val acc: {val_acc*100:.2f}%, classification_loss: {sup_loss:.2f}, discrimination_loss: {(real_loss+fake_loss)/2:.2f}, generation_loss: {gen_loss:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy \n",
    "from torch import IntTensor\n",
    "class SelfTraining: \n",
    "    def __init__(self, model: SGAN, X: Tensor, y: Tensor, num_rounds, sup_samples: int): \n",
    "        '''\n",
    "            Input of self-training model:\n",
    "            model: SGAN\n",
    "            labeled_dataset: labelled dataset \n",
    "            unlabeled_dataset: unlabelled dataset \n",
    "            num_rounds: number of self_training rounds\n",
    "        '''\n",
    "        self.model = model \n",
    "        X_sup, y_sup, X_val, y_val = self.supervised_sampling(X, y, 125)\n",
    "\n",
    "        self.labeled_dataset = CustomDataSet(X_sup, y_sup)\n",
    "        self.unlabeled_dataset = CustomDataSet(X_val, y_val)\n",
    "        self.num_rounds = num_rounds\n",
    "        self.sup_samples = sup_samples\n",
    "        \n",
    "    \n",
    "    def supervised_sampling(self, X: Tensor, y: Tensor, n_samples: int, val_ratio = 0.02):\n",
    "        num_data = y.shape[0]\n",
    "\n",
    "        ix = np.random.randint(0, num_data, n_samples)\n",
    "        \n",
    "\n",
    "        sup = ix[:int(n_samples*val_ratio)]\n",
    "        val = ix[int(n_samples*val_ratio):]\n",
    "\n",
    "        X_sup = X[sup]\n",
    "        y_sup = y[sup]\n",
    "        X_val = X[val]\n",
    "        y_val = y[val]\n",
    "\n",
    "        return X_sup, y_sup, X_val, y_val\n",
    "    def CalDisagreement(self, h1: Classify, h2: Classify, dataset: CustomDataSet): \n",
    "        '''\n",
    "            Calculate disagreement between teacher model and student model\n",
    "            h1: Teacher model \n",
    "            h2: Student model\n",
    "        '''\n",
    "        disagreement = 0\n",
    "        for x, _ in dataset: \n",
    "            disagreement += (torch.argmax(h1(x.unsqueeze(0))) == torch.argmax(h2(x.unsqueeze(0))))\n",
    "        \n",
    "        return disagreement/len(dataset)\n",
    "    def training_step(self, model: nn.Module, optimizer: optim.Optimizer, criterion: nn.modules.loss._Loss, X: Tensor, y: Tensor):\n",
    "        out: Tensor = model(X).to(dtype = torch.float)\n",
    "        y = y.to(dtype=torch.float)\n",
    "        loss: Tensor = criterion(out, y)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "\n",
    "    def random_sampling(self, sample_fraction: float, dataset: CustomDataSet, n: int): \n",
    "        dataset_set: list[CustomDataSet] = []\n",
    "        for _ in range(n): \n",
    "            \n",
    "            idx = random.sample(range(0, len(dataset)), int(len(dataset)*sample_fraction))\n",
    "            data_X = dataset.x[idx]\n",
    "            data_y = dataset.y[idx]\n",
    "\n",
    "            dataset_set.append(CustomDataSet(data_X, data_y))\n",
    "        return dataset_set\n",
    "\n",
    "    def selfTraining(self, batch_size: int, sample_fraction: float, n: int): \n",
    "        labeled_dataset = copy.copy(self.labeled_dataset)\n",
    "        unlabeled_dataset = copy.copy(self.unlabeled_dataset)\n",
    "        teacher_model = copy.copy(self.model)\n",
    "        for _ in range(self.num_rounds): \n",
    "            student_model = copy.copy(teacher_model) \n",
    "            student_model.fit(torch.cat((labeled_dataset.x, unlabeled_dataset.x), 0), \n",
    "                              torch.cat((labeled_dataset.y, unlabeled_dataset.y), 0), \n",
    "                              sup_samples=self.sup_samples, epochs = 100, batch_size = batch_size)\n",
    "            d=dict()\n",
    "            labels = []\n",
    "            for x in (unlabeled_dataset.x): \n",
    "                # print(x.get_device())\n",
    "                c_labels = student_model.classify(x.unsqueeze(0))\n",
    "                # print(c_labels[0])\n",
    "                labels.append(c_labels)\n",
    "                a = torch.sum(Tensor([i*c_labels[0][i] for i in range(len(c_labels[0]))]))\n",
    "                # print(a.shape)\n",
    "                if not isinstance(a, Tensor): \n",
    "                    print(type(a))\n",
    "                d[x] = a\n",
    "            \n",
    "            threshold = np.median(np.array(list(d.values())))\n",
    "            threshold_X = Tensor().to(device)\n",
    "            threshold_y = IntTensor().to(device)\n",
    "            threshold_idx = IntTensor().to(device)\n",
    "        \n",
    "            for i, x in enumerate(d):\n",
    "                if d[x] > threshold:\n",
    "                    threshold_X = torch.cat((threshold_X, x.unsqueeze(0)))\n",
    "                    threshold_y = torch.cat((threshold_y, Tensor(torch.argmax(labels[i]).to(device).unsqueeze(0))))\n",
    "            threshold_ds = CustomDataSet(threshold_X, threshold_y)\n",
    "            # randomly sample sample_fraction of threshold_ds\n",
    "            dataset_set = self.random_sampling(sample_fraction=sample_fraction, dataset=threshold_ds, n=n)\n",
    "            max = 0\n",
    "            for I in range(len(dataset_set)): \n",
    "                model = Classify(ConvModel(1), n_classes).to(device)\n",
    "                model.train()\n",
    "                # calculate U\\U[i]\n",
    "                unlabel = unlabeled_dataset.x\n",
    "                unlabel_i = dataset_set[I].x\n",
    "\n",
    "                counterpart = Tensor().type_as(unlabel)\n",
    "                \n",
    "                # '''debugging'''\n",
    "                # testing = True\n",
    "                # print(threshold_X.shape) \n",
    "                # for i in range(60): \n",
    "                #     for j in range(i, 61): \n",
    "                #         if torch.equal(threshold_X[i] ,threshold_X[j]): \n",
    "                #             testing = False\n",
    "                \n",
    "                # print(testing)\n",
    "                # break\n",
    "\n",
    "                for i in range(unlabel.shape[0]): \n",
    "                    check = True\n",
    "                    for j in range(unlabel_i.shape[0]): \n",
    "                       if torch.equal(unlabel[i], unlabel_i[j]):\n",
    "                           check = False\n",
    "                           break\n",
    "                    if check:\n",
    "                       counterpart = torch.cat((counterpart, unlabel[i].unsqueeze(0)))\n",
    "\n",
    "\n",
    "                # generate label of data in U\\U[i] by teacher_model classifier \n",
    "                y_counterpart = Tensor().type_as(unlabel)\n",
    "                for x in counterpart: \n",
    "                    y_counterpart = torch.cat((y_counterpart, teacher_model(x.unsqueeze(0)).unsqueeze(0)))\n",
    "\n",
    "                X_data = torch.cat((labeled_dataset.x, unlabel_i, counterpart))\n",
    "                y_data = one_hot(torch.cat((labeled_dataset.y, dataset_set[I].y ,y_counterpart)).to(dtype=torch.int))\n",
    "               \n",
    "                print(y_data.shape)\n",
    "                print(model(X_data).shape)\n",
    "\n",
    "\n",
    "                self.training_step(model, model.optimizer, model.criterion, X_data.to(device), y_data.to(device))\n",
    "                if self.CalDisagreement(student_model.classify, model, unlabeled_dataset) > max: \n",
    "                    max = self.CalDisagreement(student_model, model, unlabeled_dataset)\n",
    "                    best = dataset_set[I]\n",
    "            \n",
    "            labeled_dataset.x = torch.cat((labeled_dataset.x, best.x))\n",
    "            labeled_dataset.y = torch.cat((labeled_dataset.y, best.y))\n",
    "            # remove sample from best dataset from unlabled dataset \n",
    "            for i in range(unlabeled_dataset.x.shape[0]): \n",
    "                check = True\n",
    "                for j in range(best.x.shape[0]): \n",
    "                    if torch.equal(unlabeled_dataset.x[i], best.x[j]):\n",
    "                        check = False\n",
    "                        break\n",
    "                    if not check:\n",
    "                        unlabeled_dataset.x = torch.cat((unlabeled_dataset.x[:i], unlabeled_dataset.x[:i+1]))\n",
    "                        unlabeled_dataset.y = torch.cat((unlabeled_dataset.y[:i], unlabeled_dataset.y[:i+1]))\n",
    "            # reassign teacher model \n",
    "            teacher_model = student_model\n",
    "        \n",
    "        # return best model \n",
    "        self.model = teacher_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SGAN([1, 28, 28], n_classes, ConvModel(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "self_training = SelfTraining(model, X_train, y_train, num_rounds = 10, sup_samples = 125)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "not enough values to unpack (expected 4, got 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[122], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mself_training\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mselfTraining\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m64\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0.4\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[119], line 74\u001b[0m, in \u001b[0;36mSelfTraining.selfTraining\u001b[0;34m(self, batch_size, sample_fraction, n)\u001b[0m\n\u001b[1;32m     72\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_rounds): \n\u001b[1;32m     73\u001b[0m     student_model \u001b[38;5;241m=\u001b[39m copy\u001b[38;5;241m.\u001b[39mcopy(teacher_model) \n\u001b[0;32m---> 74\u001b[0m     \u001b[43mstudent_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlabeled_dataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43munlabeled_dataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     75\u001b[0m \u001b[43m                      \u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlabeled_dataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43munlabeled_dataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     76\u001b[0m \u001b[43m                      \u001b[49m\u001b[43msup_samples\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msup_samples\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     77\u001b[0m     d\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mdict\u001b[39m()\n\u001b[1;32m     78\u001b[0m     labels \u001b[38;5;241m=\u001b[39m []\n",
      "Cell \u001b[0;32mIn[118], line 54\u001b[0m, in \u001b[0;36mSGAN.fit\u001b[0;34m(self, X, y, sup_samples, epochs, batch_size, val)\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfit\u001b[39m(\u001b[38;5;28mself\u001b[39m, X: Tensor, y: Tensor, sup_samples, epochs \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m100\u001b[39m, batch_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m64\u001b[39m, val: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[0;32m---> 54\u001b[0m \tX_sup, y_sup, X_val, y_val \u001b[38;5;241m=\u001b[39m supervised_samples(X, y, sup_samples, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     56\u001b[0m \tdatasets \u001b[38;5;241m=\u001b[39m CustomDataSet(X, y)\n\u001b[1;32m     58\u001b[0m \tsup_datasets \u001b[38;5;241m=\u001b[39m CustomDataSet(X_sup, y_sup)\n",
      "\u001b[0;31mValueError\u001b[0m: not enough values to unpack (expected 4, got 2)"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "self_training.selfTraining(64, 0.4, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "GAN",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
