{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch import Tensor\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision.datasets import MNIST\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from CNNmodel import ConvModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_seed = 0\n",
    "torch.manual_seed(random_seed)\n",
    "random.seed(random_seed)\n",
    "np.random.seed(random_seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=0)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DATA_DIR = \"./data\"\n",
    "n_classes = 10\n",
    "num_labelled = 125\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot(y):\n",
    "\tone_hot_y = torch.zeros((len(y), 10))\n",
    "\tone_hot_y[np.arange(len(y)), y] = 1\n",
    "\treturn one_hot_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = MNIST(DATA_DIR, train = True, download=True)\n",
    "data_X = data.data.unsqueeze(1).float().to(device)\n",
    "data_X /= 255\n",
    "data_Y = one_hot(data.targets).to(device)\n",
    "\n",
    "num_data = data_Y.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def supervised_samples(X: Tensor, y: Tensor, n_samples, val_ratio = 0.2):\n",
    "\tnum_data = y.shape[0]\n",
    "\n",
    "\tix = np.random.randint(0, num_data, n_samples)\n",
    "\n",
    "\tval = ix[:int(n_samples*val_ratio)]\n",
    "\tsup = ix[int(n_samples*val_ratio):]\n",
    "\n",
    "\tX_sup = X[sup]\n",
    "\ty_sup = y[sup]\n",
    "\tX_val = X[val]\n",
    "\ty_val = y[val]\n",
    "\n",
    "\treturn X_sup, y_sup, X_val, y_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([25, 1, 28, 28])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_sup, y_sup, X_val, y_val = supervised_samples(data_X, data_Y, num_labelled)\n",
    "X_val.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataSet(Dataset):\n",
    "\tdef __init__(self, x: Tensor, y: Tensor) -> None:\n",
    "\t\tself.x: Tensor = x\n",
    "\t\tself.y: Tensor = y\n",
    "\t\tself.n_samples = len(y)\n",
    "\t\n",
    "\tdef __getitem__(self, index):\n",
    "\t\treturn self.x[index], self.y[index]\n",
    "\t\n",
    "\tdef __len__(self):\n",
    "\t\treturn self.n_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "\tdef __init__(self, inp_size, out_size) -> None:\n",
    "\t\tsuper().__init__()\n",
    "\n",
    "\n",
    "\t\tself.NN = nn.Sequential(\n",
    "\t\t\tnn.Linear(inp_size, 256*7*7),\n",
    "\t\t\tnn.LeakyReLU(negative_slope=0.2),\n",
    "\t\t)\n",
    "\n",
    "\t\tself.CONV = nn.Sequential(\n",
    "\t\t\tnn.ConvTranspose2d(256, 128, (3, 3), (2, 2)),\n",
    "\t\t\tnn.BatchNorm2d(128),\n",
    "\t\t\tnn.LeakyReLU(0.2),\n",
    "\t\t\tnn.ConvTranspose2d(128, 64, (3, 3), (1, 1)),\n",
    "\t\t\tnn.BatchNorm2d(64),\n",
    "\t\t\tnn.LeakyReLU(0.2),\n",
    "\t\t\t\n",
    "\t\t)\n",
    "\n",
    "\t\tself.out = nn.Sequential(\n",
    "\t\t\tnn.ConvTranspose2d(64, out_size[0], (3, 3), (2, 2)),\n",
    "\t\t\tnn.AdaptiveAvgPool2d((out_size[1], out_size[2])),\n",
    "\t\t\tnn.Sigmoid()\n",
    "\t\t)\n",
    "\n",
    "\t\tself.optimizer = optim.Adam(self.parameters(), lr = 0.0002, betas=[0.5, 0.999])\n",
    "\t\tself.criterion = nn.BCELoss()\n",
    "\t\n",
    "\tdef forward(self, X: Tensor):\n",
    "\t\tX = self.NN(X)\n",
    "\t\tX = X.view(-1, 256, 7, 7)\n",
    "\t\tX = self.CONV(X)\n",
    "\n",
    "\t\tX = self.out(X)\n",
    "\n",
    "\t\treturn X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Feature_Extractor(nn.Module):\n",
    "\tdef __init__(self, inp_channel) -> None:\n",
    "\t\tsuper().__init__()\n",
    "\n",
    "\n",
    "\t\tself.CNN = ConvModel(inp_channel)\n",
    "\n",
    "\t\tself.dropout = nn.Sequential(\n",
    "\t\t\tnn.Dropout(0.4)\n",
    "\t\t)\n",
    "\n",
    "\t\n",
    "\tdef forward(self, X: Tensor):\n",
    "\t\tX = self.CNN(X)\n",
    "\t\tX = self.dropout(X)\n",
    "\t\treturn X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Classify(nn.Module):\n",
    "\tdef __init__(self, feature_extractor: nn.Module, num_classes) -> None:\n",
    "\t\tsuper().__init__()\n",
    "\n",
    "\t\tself.CNN = feature_extractor\n",
    "\n",
    "\t\tself.out = nn.Sequential(\n",
    "\t\t\tnn.Linear(512, num_classes),\n",
    "\t\t\tnn.Softmax(1)\n",
    "\t\t)\n",
    "\n",
    "\t\tself.optimizer = optim.Adam(self.parameters(), lr = 0.0002, betas= [0.5, 0.999])\n",
    "\n",
    "\t\tself.criterion = nn.BCELoss()\n",
    "\t\n",
    "\t\n",
    "\tdef forward(self, X: Tensor):\n",
    "\t\tX = self.CNN(X)\n",
    "\t\tX = self.out(X)\n",
    "\n",
    "\t\treturn X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discriminator(nn.Module):\n",
    "\tdef __init__(self, feature_extractor) -> None:\n",
    "\t\tsuper().__init__()\n",
    "\t\tself.CNN = feature_extractor\n",
    "\n",
    "\t\tself.out = nn.Sequential(\n",
    "\t\t\tnn.Linear(512, 1),\n",
    "\t\t\tnn.Sigmoid()\n",
    "\t\t)\n",
    "\t\n",
    "\t\tself.optimizer = optim.Adam(self.parameters(), lr=0.0002, betas=[0.5, 0.999])\n",
    "\t\tself.criterion = nn.BCELoss()\n",
    "\n",
    "\tdef forward(self, X: Tensor):\n",
    "\t\tX = self.CNN(X)\n",
    "\n",
    "\t\tX = self.out(X)\n",
    "\n",
    "\t\treturn X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SGAN:\n",
    "\tdef __init__(self, image_size, num_classes, feature_extractor: nn.Module, latent_size = 100, lr=0.0002):\n",
    "\n",
    "\t\tself.latent_size = latent_size\n",
    "\n",
    "\t\tCNN = feature_extractor.to(device)\n",
    "\n",
    "\t\tself.generator = Generator(latent_size, image_size).to(device)\n",
    "\n",
    "\t\tself.classify = Classify(CNN, num_classes).to(device)\n",
    "\t\tself.discriminator = Discriminator(CNN).to(device)\n",
    "\n",
    "\t\tself.history = {}\n",
    "\t\n",
    "\tdef __call__(self, X: torch.Tensor):\n",
    "\t\treturn torch.argmax(self.classify(X))\n",
    "\t\n",
    "\tdef save(self, PATH = \"./\"):\n",
    "\t\ttorch.save(self.classify.state_dict(), PATH + \"/classify.pt\")\n",
    "\t\ttorch.save(self.discriminator.state_dict(), PATH + \"/discriminator.pt\")\n",
    "\t\ttorch.save(self.generator.state_dict(), PATH + \"/generator.pt\")\n",
    "\t\n",
    "\tdef validation(self, X: Tensor, y: Tensor):\n",
    "\t\tself.classify.eval()\n",
    "\n",
    "\t\tnum_data = y.shape[0]\n",
    "\n",
    "\t\trun_size = 10000\n",
    "\n",
    "\t\tcurrent = 0\n",
    "\t\tcorrect = 0\n",
    "\t\t\n",
    "\t\twhile current < num_data:\n",
    "\t\t\tcorrect += torch.count_nonzero(torch.argmax(self.classify(X[current: current + run_size]), 1) == torch.argmax(y[current: current + run_size], 1))\n",
    "\t\t\tcurrent += run_size\n",
    "\n",
    "\t\treturn (correct.float().item() / num_data)\n",
    "\n",
    "\n",
    "\tdef training_step(self, model: nn.Module, optimizer: optim.Optimizer, criterion: nn.modules.loss._Loss, X: Tensor, y: Tensor):\n",
    "\t\tout: Tensor = model(X)\n",
    "\n",
    "\t\tloss: Tensor = criterion(out, y)\n",
    "\n",
    "\t\toptimizer.zero_grad()\n",
    "\t\tloss.backward()\n",
    "\t\toptimizer.step()\n",
    "\n",
    "\t\treturn loss\n",
    "\t\n",
    "\n",
    "\tdef fit(self, X: Tensor, y: Tensor, sup_samples, epochs = 100, batch_size = 64, val: bool = False):\n",
    "\n",
    "\t\tX_sup, y_sup, X_val, y_val = supervised_samples(X, y, sup_samples)\n",
    "\n",
    "\t\tdatasets = CustomDataSet(X, y)\n",
    "\n",
    "\t\tsup_datasets = CustomDataSet(X_sup, y_sup)\n",
    "\n",
    "\t\tdataloader = DataLoader(datasets, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "\t\tsup_dataloader = DataLoader(sup_datasets, batch_size=batch_size//2, shuffle=True)\n",
    "\n",
    "\t\tfor epoch in range(epochs):\n",
    "\t\t\tself.classify.train()\n",
    "\t\t\tself.discriminator.train()\n",
    "\t\t\tself.generator.train()\n",
    "\n",
    "\t\t\t# print(f\"epoch: {epoch}\\nclassify: \")\n",
    "\t\t\t\n",
    "\t\t\t# for classify\n",
    "\t\t\tfor inputs, labels in tqdm(sup_dataloader):\n",
    "\t\t\t\tsup_loss = self.training_step(self.classify, self.classify.optimizer, self.classify.criterion, inputs.to(device), labels.to(device))\n",
    "\n",
    "\t\t\t# print(f'GAN:')\n",
    "\t\t\t# for discriminator and generator\n",
    "\t\t\tfor inputs, _ in tqdm(dataloader):\n",
    "\t\t\t\treal_loss = self.training_step(self.discriminator, self.discriminator.optimizer, self.discriminator.criterion, inputs, torch.ones((inputs.shape[0], 1)).to(device))\n",
    "\n",
    "\t\t\t\tz = torch.randn((inputs.shape[0], self.latent_size)).to(device)\n",
    "\t\t\t\tgen_out = self.generator(z)\n",
    "\t\t\t\tfake_loss = self.training_step(self.discriminator, self.discriminator.optimizer, self.discriminator.criterion, gen_out, torch.zeros(inputs.shape[0], 1).to(device))\n",
    "\t\t\t\tgen_out = self.generator(z)\n",
    "\t\t\t\tgen_loss = self.training_step(self.discriminator, self.generator.optimizer, self.discriminator.criterion, gen_out, torch.ones((inputs.shape[0], 1)).to(device))\n",
    "\n",
    "\t\t\t\n",
    "\t\t\t# train_acc = self.validation(X_sup, y_sup)\n",
    "\n",
    "\t\t\t# val_acc = self.validation(X_val, y_val)\n",
    "\t\t\t\n",
    "\t\t\t# print(f\"train acc: {train_acc*100:.2f}%, val acc: {val_acc*100:.2f}%, classification_loss: {sup_loss:.2f}, discrimination_loss: {(real_loss+fake_loss)/2:.2f}, generation_loss: {gen_loss:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 1, 28, 28])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Assuming your tensor is named 'tensor'\n",
    "tensor = torch.randn(1, 28, 28)  # Example tensor of size (1, 28, 28)\n",
    "\n",
    "# Unsqueeze the tensor\n",
    "unsqueezed_tensor = tensor.unsqueeze(1)\n",
    "\n",
    "# Check the size of the unsqueezed tensor\n",
    "print(unsqueezed_tensor.size())  # Output: torch.Size([1, 1, 28, 28])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy \n",
    "class SelfTraining: \n",
    "    def __init__(self, model: SGAN, X: Tensor, y: Tensor, num_rounds, sup_samples: int): \n",
    "        '''\n",
    "            Input of self-training model:\n",
    "            model: SGAN\n",
    "            labeled_dataset: labelled dataset \n",
    "            unlabeled_dataset: unlabelled dataset \n",
    "            num_rounds: number of self_training rounds\n",
    "        '''\n",
    "        self.model = model \n",
    "        X_sup, y_sup, X_val, y_val = self.supervised_sampling(X, y, 125)\n",
    "\n",
    "        self.labeled_dataset = CustomDataSet(X_sup, y_sup)\n",
    "        self.unlabeled_dataset = CustomDataSet(X_val, y_val)\n",
    "        self.num_rounds = num_rounds\n",
    "        self.sup_samples = sup_samples\n",
    "        \n",
    "    \n",
    "    def supervised_sampling(self, X: Tensor, y: Tensor, n_samples: int, val_ratio = 0.02):\n",
    "        num_data = y.shape[0]\n",
    "\n",
    "        print(f'n_samples is {n_samples}')\n",
    "        ix = np.random.randint(0, num_data, n_samples)\n",
    "        \n",
    "\n",
    "        sup = ix[:int(n_samples*val_ratio)]\n",
    "        val = ix[int(n_samples*val_ratio):]\n",
    "\n",
    "        X_sup = X[sup]\n",
    "        y_sup = y[sup]\n",
    "        X_val = X[val]\n",
    "        y_val = y[val]\n",
    "\n",
    "        return X_sup, y_sup, X_val, y_val\n",
    "    def CalDisagreement(self, h1: Classify, h2: Classify, dataset: CustomDataSet): \n",
    "        '''\n",
    "            Calculate disagreement between teacher model and student model\n",
    "            h1: Teacher model \n",
    "            h2: Student model\n",
    "        '''\n",
    "        disagreement = 0\n",
    "        for x, _ in dataset: \n",
    "            disagreement += (h1(x) == h2(x))\n",
    "        \n",
    "        return disagreement/len(dataset)\n",
    "    def training_step(self, model: nn.Module, optimizer: optim.Optimizer, criterion: nn.modules.loss._Loss, X: Tensor, y: Tensor):\n",
    "        out: Tensor = model(X)\n",
    "\n",
    "        loss: Tensor = criterion(out, y)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    def random_sampling(self, sample_fraction: float, dataset: CustomDataSet, n: int, teacher_model: Classify): \n",
    "        dataset_set:list[CustomDataSet] = []\n",
    "        random.seed(42) \n",
    "        for _ in range(n): \n",
    "            data_X = []\n",
    "            data_y = []\n",
    "            for i in range(0, int(len(dataset)*sample_fraction)): \n",
    "                print(len(dataset))\n",
    "                j = random.randint(0, len(dataset)-1)\n",
    "                X, y = dataset[j]\n",
    "                data_X.append(X) \n",
    "                data_y.append(torch.argmax(teacher_model(X)))\n",
    "            dataset_set.append(CustomDataSet(data_X, data_y))\n",
    "        return dataset_set\n",
    "\n",
    "    def selfTraining(self, batch_size: int, sample_fraction: float, n: int): \n",
    "        labeled_dataset = copy.copy(self.labeled_dataset)\n",
    "        unlabeled_dataset = copy.copy(self.unlabeled_dataset)\n",
    "        teacher_model = copy.copy(self.model)\n",
    "        for _ in range(self.num_rounds): \n",
    "            student_model = copy.copy(teacher_model) \n",
    "            # student_model.fit(torch.cat((labeled_dataset.x, unlabeled_dataset.x), 0), \n",
    "            #                   torch.cat((labeled_dataset.y, unlabeled_dataset.y), 0), \n",
    "            #                   sup_samples=self.sup_samples, epochs = 100, batch_size = batch_size)\n",
    "            d=dict()\n",
    "            labels = []\n",
    "            for x, _ in unlabeled_dataset: \n",
    "                x = x.unsqueeze(1)\n",
    "                c_labels = student_model.classify(x)\n",
    "                # print(c_labels[0])\n",
    "                labels.append(c_labels)\n",
    "                a = torch.sum(Tensor([i*c_labels[0][i] for i in range(len(c_labels[0]))]))\n",
    "                # print(a.shape)\n",
    "                if not isinstance(a, Tensor): \n",
    "                    print(type(a))\n",
    "                d[x] = a\n",
    "            \n",
    "            threshold = np.median(np.array(list(d.values())))\n",
    "            threshold_X = []\n",
    "            threshold_y = []\n",
    "        \n",
    "            for i, x in enumerate(d):\n",
    "                if d[x] > threshold: \n",
    "                    threshold_X.append(x)\n",
    "                    threshold_y.append(torch.argmax(labels[i]))\n",
    "            threshold_ds = CustomDataSet(threshold_X, threshold_y)\n",
    "            # randomly sample sample_fraction of threshold_ds\n",
    "            dataset_set = self.random_sampling(sample_fraction=sample_fraction, dataset=threshold_ds, n=n, teacher_model=teacher_model)\n",
    "            max = 0\n",
    "            for i in range(n): \n",
    "                model = Classify(ConvModel(1), n_classes)\n",
    "                model.train()\n",
    "                # calculate U\\U[i]\n",
    "                unlabel = set(unlabeled_dataset.x)\n",
    "                unlabel_i = set(dataset_set[i].x) \n",
    "                counterpart = list(unlabel - unlabel_i)\n",
    "                # print(type(labeled_dataset.x), type(dataset_set[i].x), type(counterpart))\n",
    "                X_data = labeled_dataset.x + Tensor(dataset_set[i].x) + Tensor(counterpart)\n",
    "                y_data = labeled_dataset.y + dataset_set[i].y + [torch.argmax(teacher_model(x)) for x in counterpart]\n",
    "                \n",
    "                self.training_step(model, model.optimizer, model.criterion, X_data.to(device), y_data.to(device))\n",
    "                if self.CalDisagreement(student_model, model, unlabeled_dataset) > max: \n",
    "                    max = self.CalDisagreement(student_model, model, unlabeled_dataset)\n",
    "                    best = dataset_set[i]\n",
    "            \n",
    "            labeled_dataset.x = list(set(labeled_dataset.x) + set(best.x))\n",
    "            labeled_dataset.y = list(set(labeled_dataset.y) + set(best.y))\n",
    "            # reassign teacher model \n",
    "            teacher_model = student_model\n",
    "        \n",
    "        # return best model \n",
    "        self.model = teacher_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SGAN([1, 28, 28], n_classes, ConvModel(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_samples is 125\n"
     ]
    }
   ],
   "source": [
    "self_training = SelfTraining(model, data_X, data_Y, num_rounds = 10, sup_samples = 125)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "61\n",
      "61\n",
      "61\n",
      "61\n",
      "61\n",
      "61\n",
      "61\n",
      "61\n",
      "61\n",
      "61\n",
      "61\n",
      "61\n",
      "61\n",
      "61\n",
      "61\n",
      "61\n",
      "61\n",
      "61\n",
      "61\n",
      "61\n",
      "61\n",
      "61\n",
      "61\n",
      "61\n",
      "61\n",
      "61\n",
      "61\n",
      "61\n",
      "61\n",
      "61\n",
      "61\n",
      "61\n",
      "61\n",
      "61\n",
      "61\n",
      "61\n",
      "61\n",
      "61\n",
      "61\n",
      "61\n",
      "61\n",
      "61\n",
      "61\n",
      "61\n",
      "61\n",
      "61\n",
      "61\n",
      "61\n",
      "61\n",
      "61\n",
      "61\n",
      "61\n",
      "61\n",
      "61\n",
      "61\n",
      "61\n",
      "61\n",
      "61\n",
      "61\n",
      "61\n",
      "61\n",
      "61\n",
      "61\n",
      "61\n",
      "61\n",
      "61\n",
      "61\n",
      "61\n",
      "61\n",
      "61\n",
      "61\n",
      "61\n",
      "61\n",
      "61\n",
      "61\n",
      "61\n",
      "61\n",
      "61\n",
      "61\n",
      "61\n",
      "61\n",
      "61\n",
      "61\n",
      "61\n",
      "61\n",
      "61\n",
      "61\n",
      "61\n",
      "61\n",
      "61\n",
      "61\n",
      "61\n",
      "61\n",
      "61\n",
      "61\n",
      "61\n",
      "61\n",
      "61\n",
      "61\n",
      "61\n",
      "61\n",
      "61\n",
      "61\n",
      "61\n",
      "61\n",
      "61\n",
      "61\n",
      "61\n",
      "61\n",
      "61\n",
      "61\n",
      "61\n",
      "61\n",
      "61\n",
      "61\n",
      "61\n",
      "61\n",
      "61\n",
      "61\n",
      "61\n",
      "61\n",
      "61\n",
      "61\n",
      "61\n",
      "61\n",
      "61\n",
      "61\n",
      "61\n",
      "61\n",
      "61\n",
      "61\n",
      "61\n",
      "61\n",
      "61\n",
      "61\n",
      "61\n",
      "61\n",
      "61\n",
      "61\n",
      "61\n",
      "61\n",
      "61\n",
      "61\n",
      "61\n",
      "61\n",
      "61\n",
      "61\n",
      "61\n",
      "61\n",
      "61\n",
      "61\n",
      "61\n",
      "61\n",
      "61\n",
      "61\n",
      "61\n",
      "61\n",
      "61\n",
      "61\n",
      "61\n",
      "61\n",
      "61\n",
      "61\n",
      "61\n",
      "61\n",
      "61\n",
      "61\n",
      "61\n",
      "61\n",
      "61\n",
      "61\n",
      "61\n",
      "61\n",
      "61\n",
      "61\n",
      "61\n",
      "61\n",
      "61\n",
      "61\n",
      "61\n",
      "61\n",
      "61\n",
      "61\n",
      "61\n",
      "61\n",
      "61\n",
      "61\n",
      "61\n",
      "61\n",
      "61\n",
      "61\n",
      "61\n",
      "61\n",
      "61\n",
      "61\n",
      "61\n",
      "61\n",
      "61\n",
      "61\n",
      "61\n",
      "61\n",
      "61\n",
      "61\n",
      "61\n",
      "61\n",
      "61\n",
      "61\n",
      "61\n",
      "61\n",
      "61\n",
      "61\n",
      "61\n",
      "61\n",
      "61\n",
      "61\n",
      "61\n",
      "61\n",
      "61\n",
      "61\n",
      "61\n",
      "61\n",
      "61\n",
      "61\n",
      "61\n",
      "61\n",
      "61\n",
      "61\n",
      "61\n",
      "61\n",
      "61\n",
      "61\n",
      "61\n",
      "61\n",
      "61\n",
      "61\n",
      "61\n",
      "61\n",
      "61\n",
      "61\n",
      "61\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "only one element tensors can be converted to Python scalars",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[63], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mself_training\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mselfTraining\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m64\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0.4\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[60], line 113\u001b[0m, in \u001b[0;36mSelfTraining.selfTraining\u001b[0;34m(self, batch_size, sample_fraction, n)\u001b[0m\n\u001b[1;32m    111\u001b[0m counterpart \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(unlabel \u001b[38;5;241m-\u001b[39m unlabel_i)\n\u001b[1;32m    112\u001b[0m \u001b[38;5;66;03m# print(type(labeled_dataset.x), type(dataset_set[i].x), type(counterpart))\u001b[39;00m\n\u001b[0;32m--> 113\u001b[0m X_data \u001b[38;5;241m=\u001b[39m labeled_dataset\u001b[38;5;241m.\u001b[39mx \u001b[38;5;241m+\u001b[39m \u001b[43mTensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset_set\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m+\u001b[39m Tensor(counterpart)\n\u001b[1;32m    114\u001b[0m y_data \u001b[38;5;241m=\u001b[39m labeled_dataset\u001b[38;5;241m.\u001b[39my \u001b[38;5;241m+\u001b[39m dataset_set[i]\u001b[38;5;241m.\u001b[39my \u001b[38;5;241m+\u001b[39m [torch\u001b[38;5;241m.\u001b[39margmax(teacher_model(x)) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m counterpart]\n\u001b[1;32m    116\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining_step(model, model\u001b[38;5;241m.\u001b[39moptimizer, model\u001b[38;5;241m.\u001b[39mcriterion, X_data\u001b[38;5;241m.\u001b[39mto(device), y_data\u001b[38;5;241m.\u001b[39mto(device))\n",
      "\u001b[0;31mValueError\u001b[0m: only one element tensors can be converted to Python scalars"
     ]
    }
   ],
   "source": [
    "self_training.selfTraining(64, 0.4, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(data_X, data_Y, sup_samples=num_labelled, epochs=100, batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.74225"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.validation(data_X, data_Y)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "GAN",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
